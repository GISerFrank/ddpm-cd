import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒ
import torch.multiprocessing as mp  # å¤šè¿›ç¨‹
from torch.nn.parallel import DistributedDataParallel as DDP  # åˆ†å¸ƒå¼DataParallel
from torch.utils.data.distributed import DistributedSampler  # åˆ†å¸ƒå¼é‡‡æ ·å™¨

import data as Data
import model as Model
import argparse
import logging
import core.logger as Logger
import core.metrics as Metrics
from core.wandb_logger import WandbLogger
from tensorboardX import SummaryWriter
import os
import numpy as np
from model.cd_modules.cd_head import cd_head 
from misc.print_diffuse_feats import print_feats
import time
from contextlib import contextmanager

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'  # ä½¿ç”¨4ä¸ªGPU
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥CUDAè°ƒç”¨ï¼Œä¾¿äºè°ƒè¯•

# ==================== ä¼˜åŒ–ç‰ˆæ ‡ç­¾éªŒè¯å™¨ ====================
class LabelValidator:
    """é«˜æ•ˆæ ‡ç­¾éªŒè¯å™¨ - å•ä¾‹æ¨¡å¼"""
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.is_normalized = None
            self.validation_done = False
            self.label_stats = {}
            LabelValidator._initialized = True
    
    def validate_and_fix_labels(self, data, phase="train"):
        """
        é«˜æ•ˆæ ‡ç­¾éªŒè¯ - åªåœ¨ç¬¬ä¸€æ¬¡è¯¦ç»†æ£€æŸ¥ï¼Œåç»­å¿«é€Ÿå¤„ç†
        """
        if 'L' not in data:
            return False
        
        labels = data['L']
        
        # å¿«é€Ÿé€šé“ï¼šå¦‚æœå·²ç»éªŒè¯è¿‡ï¼Œç›´æ¥å¤„ç†
        if self.validation_done:
            if self.is_normalized:
                data['L'] = (labels >= 0.5).long()
            else:
                fixed_labels = labels.clone()
                unique_vals = torch.unique(labels)
                if 255 in unique_vals:
                    fixed_labels[labels == 255] = 1
                data['L'] = torch.clamp(fixed_labels, 0, 1).long()
            return True
        
        # ç¬¬ä¸€æ¬¡è¯¦ç»†éªŒè¯
        unique_vals = torch.unique(labels)
        min_val = labels.min().item()
        max_val = labels.max().item()
        
        print(f"\nğŸ” [{phase}] æ ‡ç­¾éªŒè¯ï¼ˆä»…æ˜¾ç¤ºä¸€æ¬¡ï¼‰:")
        print(f"   å½¢çŠ¶: {labels.shape}, æ•°æ®ç±»å‹: {labels.dtype}")
        print(f"   å€¼èŒƒå›´: [{min_val}, {max_val}]")
        print(f"   å”¯ä¸€å€¼: {unique_vals.tolist()}")
        
        # åˆ¤æ–­æ ‡ç­¾ç±»å‹
        self.is_normalized = (max_val <= 1.0 and min_val >= 0.0)
        
        if self.is_normalized:
            print(f"   ğŸ”§ æ£€æµ‹åˆ°å½’ä¸€åŒ–æ ‡ç­¾ï¼Œä½¿ç”¨é˜ˆå€¼äºŒå€¼åŒ–ï¼ˆé˜ˆå€¼=0.5ï¼‰")
            fixed_labels = (labels >= 0.5).long()
        else:
            print(f"   ğŸ”§ æ£€æµ‹åˆ°æ ‡å‡†æ ‡ç­¾ï¼Œæ˜ å°„255â†’1")
            fixed_labels = labels.clone()
            if 255 in unique_vals:
                fixed_labels[labels == 255] = 1
            fixed_labels = torch.clamp(fixed_labels, 0, 1).long()
        
        # éªŒè¯ä¿®å¤ç»“æœ
        final_unique = torch.unique(fixed_labels)
        zero_count = (fixed_labels == 0).sum().item()
        one_count = (fixed_labels == 1).sum().item()
        total = zero_count + one_count
        
        print(f"   âœ… ä¿®å¤å®Œæˆ: å”¯ä¸€å€¼{final_unique.tolist()}")
        print(f"   ğŸ“Š åƒç´ åˆ†å¸ƒ: æ— å˜åŒ–={100*zero_count/total:.1f}%, æœ‰å˜åŒ–={100*one_count/total:.1f}%")
        print(f"   âœ… æ ‡ç­¾éªŒè¯è®¾ç½®å®Œæˆï¼Œåç»­æ‰¹æ¬¡å°†å¿«é€Ÿå¤„ç†\n")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self.label_stats = {
            'zero_ratio': zero_count / total,
            'one_ratio': one_count / total,
            'is_normalized': self.is_normalized
        }
        
        data['L'] = fixed_labels
        self.validation_done = True
        return True

# å…¨å±€æ ‡ç­¾éªŒè¯å™¨
label_validator = LabelValidator()

# ==================== å†…å­˜ç®¡ç†å·¥å…· ====================
@contextmanager
def memory_efficient_context():
    """å†…å­˜ç®¡ç†ä¸Šä¸‹æ–‡"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    try:
        yield
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    def __init__(self):
        self.step_times = []
        self.memory_usage = []
        self.start_time = time.time()
    
    def log_step(self, step_time, memory_mb=None):
        self.step_times.append(step_time)
        if memory_mb:
            self.memory_usage.append(memory_mb)
    
    def get_stats(self):
        if not self.step_times:
            return "æ— ç»Ÿè®¡æ•°æ®"
        
        avg_time = np.mean(self.step_times[-100:])  # æœ€è¿‘100æ­¥å¹³å‡
        total_time = time.time() - self.start_time
        
        stats = f"å¹³å‡æ­¥æ—¶: {avg_time:.2f}s, æ€»æ—¶é—´: {total_time/60:.1f}min"
        
        if self.memory_usage:
            avg_memory = np.mean(self.memory_usage[-10:])
            stats += f", æ˜¾å­˜: {avg_memory:.1f}MB"
        
        return stats

# ==================== ä¼˜åŒ–ç‰ˆç‰¹å¾é‡æ’ ====================
def apply_feature_reordering_optimized(f_A, f_B, train_data, change_detection, opt, phase="train"):
    """
    å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾é‡æ’æ–¹æ¡ˆ
    """
    try:
        feat_scales = opt['model_cd']['feat_scales']  # [2, 5, 8, 11, 14]
        cd_expected_order = sorted(feat_scales, reverse=True)  # [14, 11, 8, 5, 2]
        
        # åªåœ¨ç¬¬ä¸€æ¬¡æ˜¾ç¤ºä¿¡æ¯
        if not hasattr(apply_feature_reordering_optimized, '_logged'):
            print("ğŸ¯ ä½¿ç”¨ä¼˜åŒ–çš„ç‰¹å¾é‡æ’æ–¹æ¡ˆ")
            print("   ä¿æŒåŸå§‹å¤šå°ºåº¦é…ç½®çš„å®Œæ•´è¯­ä¹‰")
            for i, scale in enumerate(cd_expected_order):
                print(f"     Block{i}: ä½¿ç”¨layer{scale}ç‰¹å¾")
            apply_feature_reordering_optimized._logged = True
        
        # é«˜æ•ˆé‡æ’ï¼šç›´æ¥åœ¨åŸåœ°ä¿®æ”¹
        reordered_f_A = []
        reordered_f_B = []
        
        for fa, fb in zip(f_A, f_B):
            if isinstance(fa, list) and len(fa) > max(feat_scales):
                timestep_A = [fa[scale] for scale in cd_expected_order]
                timestep_B = [fb[scale] for scale in cd_expected_order]
                reordered_f_A.append(timestep_A)
                reordered_f_B.append(timestep_B)
            else:
                raise ValueError(f"ç‰¹å¾æ ¼å¼é”™è¯¯: æœŸæœ›listé•¿åº¦>{max(feat_scales)}, å®é™…{type(fa)}")
        
        # æ¸…ç†åŸå§‹ç‰¹å¾é‡Šæ”¾å†…å­˜
        del f_A, f_B
        
        # ä½¿ç”¨é‡æ’åçš„ç‰¹å¾
        change_detection.feed_data(reordered_f_A, reordered_f_B, train_data)
        
        # æ¸…ç†é‡æ’åçš„ç‰¹å¾
        del reordered_f_A, reordered_f_B
        
        return True
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾é‡æ’å¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ¡ˆ...")
        
        # ç®€åŒ–å›é€€æ–¹æ¡ˆ
        target_layers = [12, 13, 14]
        corrected_f_A = []
        corrected_f_B = []
        
        for fa, fb in zip(f_A, f_B):
            timestep_A = [fa[i] for i in target_layers if i < len(fa)]
            timestep_B = [fb[i] for i in target_layers if i < len(fb)]
            corrected_f_A.append(timestep_A)
            corrected_f_B.append(timestep_B)
        
        del f_A, f_B
        change_detection.feed_data(corrected_f_A, corrected_f_B, train_data)
        del corrected_f_A, corrected_f_B
        
        return False

# ==================== è®­ç»ƒä¼˜åŒ–è®¾ç½® ====================
def setup_training_optimization(diffusion, change_detection):
    """è®¾ç½®è®­ç»ƒä¼˜åŒ–"""
    print("ğŸš€ è®¾ç½®è®­ç»ƒä¼˜åŒ–...")
    
    # å¯ç”¨CUDAä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # æ£€æŸ¥æ··åˆç²¾åº¦æ”¯æŒ
    use_amp = False
    if torch.cuda.is_available():
        try:
            from torch.cuda.amp import autocast, GradScaler
            use_amp = True
            print("   âœ… æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ")
        except ImportError:
            print("   âš ï¸  ä¸æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ")
    
    # è®¾ç½®diffusionæ¨¡å‹ä¸ºevalæ¨¡å¼ï¼ˆå¦‚æœä¸éœ€è¦è®­ç»ƒï¼‰
    if hasattr(diffusion.netG, 'eval'):
        diffusion.netG.eval()
        print("   âœ… Diffusionæ¨¡å‹è®¾ç½®ä¸ºevalæ¨¡å¼")
    
    # æ£€æŸ¥å¤šGPUè®¾ç½®
    if torch.cuda.device_count() > 1:
        print(f"   âœ… æ£€æµ‹åˆ°{torch.cuda.device_count()}ä¸ªGPU")
        
        # æ˜¾ç¤ºGPUçŠ¶æ€
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1024**3
            print(f"     GPU {i}: {props.name} ({memory_gb:.1f}GB)")
    
    print("ğŸš€ è®­ç»ƒä¼˜åŒ–è®¾ç½®å®Œæˆ\n")
    
    return use_amp

# ==================== æ‰¹é‡å¤„ç†ä¼˜åŒ– ====================
# def process_batch_efficiently(train_data, diffusion, change_detection, opt, phase="train"):
#     """é«˜æ•ˆçš„æ‰¹é‡å¤„ç†"""
#     with memory_efficient_context():
#         # 1. å¿«é€Ÿæ ‡ç­¾éªŒè¯
#         label_validator.validate_and_fix_labels(train_data, phase)
        
#         # 2. ç‰¹å¾æå–
#         diffusion.feed_data(train_data)
        
#         # 3. æ”¶é›†ç‰¹å¾
#         f_A, f_B = [], []
#         for t in opt['model_cd']['t']:
#             fe_A_t, fd_A_t, fe_B_t, fd_B_t = diffusion.get_feats(t=t)
#             if opt['model_cd']['feat_type'] == "dec":
#                 f_A.append(fd_A_t)
#                 f_B.append(fd_B_t)
#                 del fe_A_t, fe_B_t  # ç«‹å³æ¸…ç†
#             else:
#                 f_A.append(fe_A_t)
#                 f_B.append(fe_B_t)
#                 del fd_A_t, fd_B_t  # ç«‹å³æ¸…ç†
        
#         # 4. ç‰¹å¾é‡æ’
#         apply_feature_reordering_optimized(f_A, f_B, train_data, change_detection, opt, phase)
def process_batch_efficiently(train_data, diffusion, change_detection, opt, phase="train"):
    """é«˜æ•ˆçš„æ‰¹é‡å¤„ç† - ä¿®å¤è®¾å¤‡é—®é¢˜"""
    with memory_efficient_context():
        # 1. å¿«é€Ÿæ ‡ç­¾éªŒè¯
        label_validator.validate_and_fix_labels(train_data, phase)
        
        # 2. ğŸ”§ å¼ºåˆ¶è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥
        device = None
        try:
            device = next(diffusion.netG.parameters()).device
        except:
            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        # ç¡®ä¿æ‰€æœ‰æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        for key, value in train_data.items():
            if torch.is_tensor(value):
                train_data[key] = value.to(device)
        
        # 3. ç¡®ä¿diffusionæ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡
        if isinstance(diffusion.netG, nn.DataParallel):
            actual_model = diffusion.netG.module
            if next(actual_model.parameters()).device != device:
                actual_model = actual_model.to(device)
                diffusion.netG = nn.DataParallel(actual_model, device_ids=[0, 1, 2, 3])
        else:
            if next(diffusion.netG.parameters()).device != device:
                diffusion.netG = diffusion.netG.to(device)
        
        print(f"ğŸ” [{phase}] è®¾å¤‡æ£€æŸ¥ - æ¨¡å‹: {next(diffusion.netG.parameters()).device}, æ•°æ®: {train_data['A'].device}")
        
        # 4. ç‰¹å¾æå–
        diffusion.feed_data(train_data)
        
        # 5. æ”¶é›†ç‰¹å¾
        f_A, f_B = [], []
        for t in opt['model_cd']['t']:
            fe_A_t, fd_A_t, fe_B_t, fd_B_t = diffusion.get_feats(t=t)
            if opt['model_cd']['feat_type'] == "dec":
                f_A.append(fd_A_t)
                f_B.append(fd_B_t)
                del fe_A_t, fe_B_t
            else:
                f_A.append(fe_A_t)
                f_B.append(fe_B_t)
                del fd_A_t, fd_B_t
        
        # 6. ç‰¹å¾é‡æ’
        apply_feature_reordering_optimized(f_A, f_B, train_data, change_detection, opt, phase)

# ==================== ä¼˜åŒ–çš„æ—¥å¿—ç®¡ç† ====================
def optimized_logging(current_step, current_epoch, n_epoch, loader, change_detection, 
                     logger, opt, phase="train", performance_monitor=None):
    """ä¼˜åŒ–çš„æ—¥å¿—è¾“å‡º"""
    # åŠ¨æ€è°ƒæ•´æ—¥å¿—é¢‘ç‡
    if phase == "train":
        base_freq = opt['train'].get('train_print_freq', 10)
        log_freq = max(base_freq, len(loader) // 1000)  # è‡³å°‘æ¯5%æ˜¾ç¤ºä¸€æ¬¡
    else:
        log_freq = max(1, len(loader) // 500)  # éªŒè¯æ—¶æ¯10%æ˜¾ç¤ºä¸€æ¬¡
    
    if current_step % log_freq == 0:
        try:
            logs = change_detection.get_current_log()
            
            # åŸºç¡€ä¿¡æ¯
            progress = f"[{current_epoch}/{n_epoch-1}]"
            step_info = f"Step {current_step}/{len(loader)}"
            metrics = f"Loss: {logs.get('l_cd', 0):.5f} mF1: {logs.get('running_acc', 0):.5f}"
            
            # æ€§èƒ½ä¿¡æ¯
            perf_info = ""
            if performance_monitor:
                perf_info = f" | {performance_monitor.get_stats()}"
            
            message = f"{progress} {step_info} {metrics}{perf_info}"
            print(message)
            
        except Exception as e:
            print(f"æ—¥å¿—è¾“å‡ºé”™è¯¯: {e}")

# ==================== é”™è¯¯å¤„ç†è£…é¥°å™¨ ====================
def safe_training_step(func):
    """å®‰å…¨è®­ç»ƒæ­¥éª¤è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except RuntimeError as e:
            if "device-side assert triggered" in str(e) or "CUDA" in str(e):
                print(f"âš ï¸  CUDAé”™è¯¯å·²è‡ªåŠ¨å¤„ç†: {str(e)[:100]}...")
                torch.cuda.empty_cache()
                return False
            else:
                raise
        except Exception as e:
            print(f"âŒ è®­ç»ƒæ­¥éª¤é”™è¯¯: {e}")
            return False
    return wrapper

@safe_training_step
def execute_training_step(change_detection):
    """æ‰§è¡Œè®­ç»ƒæ­¥éª¤"""
    change_detection.optimize_parameters()
    change_detection._collect_running_batch_states()
    return True

# ==================== ä¸»å‡½æ•° ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', type=str, default='config/ddpm_cd.json',
                        help='JSON file for configuration')
    parser.add_argument('-p', '--phase', type=str, choices=['train', 'test'],
                        help='Run either train(training + validation) or testing', default='train')
    parser.add_argument('-gpu', '--gpu_ids', type=str, default=None)
    parser.add_argument('-debug', '-d', action='store_true')
    parser.add_argument('-enable_wandb', action='store_true')
    parser.add_argument('-log_eval', action='store_true')

    # è§£æé…ç½®
    args = parser.parse_args()
    opt = Logger.parse(args)
    opt = Logger.dict_to_nonedict(opt)

    # è®¾ç½®æ—¥å¿—
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True

    Logger.setup_logger(None, opt['path']['log'], 'train', level=logging.INFO, screen=True)
    Logger.setup_logger('test', opt['path']['log'], 'test', level=logging.INFO)
    logger = logging.getLogger('base')
    logger.info(Logger.dict2str(opt))
    tb_logger = SummaryWriter(log_dir=opt['path']['tb_logger'])

    # åˆå§‹åŒ–WandbLogger
    if opt['enable_wandb']:
        import wandb
        print("Initializing wandblog.")
        wandb_logger = WandbLogger(opt)
        wandb.define_metric('epoch')
        wandb.define_metric('training/train_step')
        wandb.define_metric("training/*", step_metric="train_step")
        wandb.define_metric('validation/val_step')
        wandb.define_metric("validation/*", step_metric="val_step")
        train_step = 0
        val_step = 0
    else:
        wandb_logger = None

    # åŠ è½½æ•°æ®é›†
    print("ğŸ”„ åŠ è½½æ•°æ®é›†...")
    for phase, dataset_opt in opt['datasets'].items():
        if phase == 'train' and args.phase != 'test':
            print("Creating [train] change-detection dataloader.")
            train_set = Data.create_cd_dataset(dataset_opt, phase)
            train_loader = Data.create_cd_dataloader(train_set, dataset_opt, phase)
            opt['len_train_dataloader'] = len(train_loader)

        elif phase == 'val' and args.phase != 'test':
            print("Creating [val] change-detection dataloader.")
            val_set = Data.create_cd_dataset(dataset_opt, phase)
            val_loader = Data.create_cd_dataloader(val_set, dataset_opt, phase)
            opt['len_val_dataloader'] = len(val_loader)
        
        elif phase == 'test' and args.phase == 'test':
            print("Creating [test] change-detection dataloader.")
            test_set = Data.create_cd_dataset(dataset_opt, phase)
            test_loader = Data.create_cd_dataloader(test_set, dataset_opt, phase)
            opt['len_test_dataloader'] = len(test_loader)
    
    logger.info('Initial Dataset Finished')

    # åŠ è½½æ¨¡å‹
    print("ğŸ”„ åŠ è½½æ‰©æ•£æ¨¡å‹...")
    diffusion = Model.create_model(opt)
    logger.info('Initial Diffusion Model Finished')

    # å¤„ç†DataParallel
    if isinstance(diffusion.netG, nn.DataParallel):
        diffusion.netG = diffusion.netG.module
        print("å·²è§£åŒ…diffusionæ¨¡å‹çš„DataParallel")

    # å¤šGPUè®¾ç½®
    if torch.cuda.device_count() > 1:
        print(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
        diffusion.netG = diffusion.netG.cuda()
        diffusion.netG = nn.DataParallel(diffusion.netG, device_ids=[0, 1, 2, 3])
        
        # é€‚åº¦å¢åŠ batch size
        for phase in opt['datasets']:
            if 'batch_size' in opt['datasets'][phase]:
                original_bs = opt['datasets'][phase]['batch_size']
                # å¯ä»¥æ ¹æ®GPUæ•°é‡è°ƒæ•´
                # opt['datasets'][phase]['batch_size'] = original_bs * 2
                print(f"{phase} batch_size: {original_bs}")

    # è®¾ç½®å™ªå£°è°ƒåº¦
    diffusion.set_new_noise_schedule(
        opt['model']['beta_schedule'][opt['phase']], schedule_phase=opt['phase'])
    
    # åˆ›å»ºå˜åŒ–æ£€æµ‹æ¨¡å‹
    print("ğŸ”„ åŠ è½½å˜åŒ–æ£€æµ‹æ¨¡å‹...")
    change_detection = Model.create_CD_model(opt)
    
    # ğŸ”§ å¼ºåˆ¶è®¾å¤‡ä¸€è‡´æ€§è®¾ç½®
    if torch.cuda.is_available():
        target_device = torch.device('cuda:0')
        
        # ç¡®ä¿æ‰©æ•£æ¨¡å‹åœ¨GPU
        if isinstance(diffusion.netG, nn.DataParallel):
            diffusion.netG.module = diffusion.netG.module.to(target_device)
        else:
            diffusion.netG = diffusion.netG.to(target_device)
        
        # ç¡®ä¿å˜åŒ–æ£€æµ‹æ¨¡å‹åœ¨GPU
        if hasattr(change_detection, 'netCD') and change_detection.netCD is not None:
            if isinstance(change_detection.netCD, nn.DataParallel):
                change_detection.netCD.module = change_detection.netCD.module.to(target_device)
            else:
                change_detection.netCD = change_detection.netCD.to(target_device)
        
        print(f"âœ… å¼ºåˆ¶è®¾å¤‡è®¾ç½®å®Œæˆ: {target_device}")
    
    # å¤„ç†CDæ¨¡å‹çš„DataParallel
    if hasattr(change_detection, 'netCD') and change_detection.netCD is not None:
        if isinstance(change_detection.netCD, nn.DataParallel):
            change_detection.netCD = change_detection.netCD.module
            print("å·²è§£åŒ…CDæ¨¡å‹çš„DataParallel")
        
        if torch.cuda.device_count() > 1:
            change_detection.netCD = change_detection.netCD.cuda()

    # è®¾ç½®è®­ç»ƒä¼˜åŒ–
    use_amp = setup_training_optimization(diffusion, change_detection)
    
    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    performance_monitor = PerformanceMonitor()

    print("ğŸš€ æ‰€æœ‰è®¾ç½®å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...\n")

    #################
    # è®­ç»ƒå¾ªç¯ #
    #################
    n_epoch = opt['train']['n_epoch']
    best_mF1 = 0.0
    start_epoch = 0

    if opt['phase'] == 'train':
        # éªŒè¯è®¾å¤‡è®¾ç½®
        device = next(diffusion.netG.parameters()).device
        print(f"è®¾å¤‡æ£€æŸ¥: æ¨¡å‹åœ¨ {device}")
        
        if device.type == 'cpu' and torch.cuda.is_available():
            target_device = torch.device('cuda:0')
            print(f"å¼ºåˆ¶å°†æ¨¡å‹ä» {device} ç§»åŠ¨åˆ° {target_device}")
            diffusion.netG = diffusion.netG.to(target_device)
            change_detection.netCD = change_detection.netCD.to(target_device)
            device = next(diffusion.netG.parameters()).device
            print(f"ç§»åŠ¨åéªŒè¯: æ¨¡å‹ç°åœ¨åœ¨ {device}")

        for current_epoch in range(start_epoch, n_epoch):
            epoch_start_time = time.time()
            change_detection._clear_cache()
            
            train_result_path = f'{opt["path"]["results"]}/train/{current_epoch}'
            os.makedirs(train_result_path, exist_ok=True)
            
            ################
            ### è®­ç»ƒé˜¶æ®µ ###
            ################
            print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ Epoch {current_epoch}/{n_epoch-1}")
            message = f'å­¦ä¹ ç‡: {change_detection.optCD.param_groups[0]["lr"]:.7f}'
            logger.info(message)
            
            for current_step, train_data in enumerate(train_loader):
                step_start_time = time.time()
                
                # é«˜æ•ˆæ‰¹é‡å¤„ç†
                process_batch_efficiently(train_data, diffusion, change_detection, opt, "train")
                
                # å®‰å…¨çš„è®­ç»ƒæ­¥éª¤
                success = execute_training_step(change_detection)
                
                if not success:
                    print(f"è·³è¿‡æ­¥éª¤ {current_step}")
                    continue
                
                # è®°å½•æ€§èƒ½
                step_time = time.time() - step_start_time
                memory_mb = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0
                performance_monitor.log_step(step_time, memory_mb)
                
                # ä¼˜åŒ–çš„æ—¥å¿—è¾“å‡º
                optimized_logging(current_step, current_epoch, n_epoch, train_loader, 
                                change_detection, logger, opt, "train", performance_monitor)
                
                # ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆå‡å°‘é¢‘ç‡ï¼‰
                save_freq = max(opt['train']['train_print_freq'] * 2, len(train_loader) // 5)
                if current_step % save_freq == 0:
                    try:
                        visuals = change_detection.get_current_visuals()
                        
                        # ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
                        device = train_data['A'].device
                        visuals['pred_cm'] = visuals['pred_cm'] * 2.0 - 1.0
                        visuals['gt_cm'] = visuals['gt_cm'] * 2.0 - 1.0
                        
                        pred_cm_expanded = visuals['pred_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
                        gt_cm_expanded = visuals['gt_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
                        
                        grid_img = torch.cat((train_data['A'], train_data['B'], 
                                            pred_cm_expanded, gt_cm_expanded), dim=0)
                        grid_img = Metrics.tensor2img(grid_img)
                        
                        Metrics.save_img(grid_img, 
                            f'{train_result_path}/img_A_B_pred_gt_e{current_epoch}_b{current_step}.png')
                    except Exception as e:
                        print(f"ä¿å­˜å¯è§†åŒ–å¤±è´¥: {e}")
                
                # å®šæœŸå†…å­˜æ¸…ç†
                if current_step % 50 == 0:
                    torch.cuda.empty_cache()
            
            ### è®­ç»ƒepochæ€»ç»“ ###
            try:
                change_detection._collect_epoch_states()
                logs = change_detection.get_current_log()
                
                epoch_time = time.time() - epoch_start_time
                message = f'[è®­ç»ƒ Epoch {current_epoch}] mF1={logs["epoch_acc"]:.5f}, ' \
                         f'ç”¨æ—¶={epoch_time/60:.1f}åˆ†é’Ÿ'
                
                print(f"\nâœ… {message}")
                logger.info(message)
                
                # è¯¦ç»†æŒ‡æ ‡
                for k, v in logs.items():
                    tb_logger.add_scalar(f'train/{k}', v, current_epoch)
                
                if wandb_logger:
                    wandb_logger.log_metrics({
                        'training/mF1': logs['epoch_acc'],
                        'training/mIoU': logs['miou'],
                        'training/OA': logs['acc'],
                        'training/change-F1': logs['F1_1'],
                        'training/no-change-F1': logs['F1_0'],
                        'training/change-IoU': logs['iou_1'],
                        'training/no-change-IoU': logs['iou_0'],
                        'training/train_step': current_epoch,
                        'training/loss': logs.get('l_cd'),
                    })
                    
            except Exception as e:
                print(f"è®­ç»ƒæŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
            
            change_detection._clear_cache()
            change_detection._update_lr_schedulers()
            
            ##################
            ### éªŒè¯é˜¶æ®µ ###
            ##################
            if current_epoch % opt['train']['val_freq'] == 0:
                print(f"\nğŸ” å¼€å§‹éªŒè¯ Epoch {current_epoch}")
                val_start_time = time.time()
                
                val_result_path = f'{opt["path"]["results"]}/val/{current_epoch}'
                os.makedirs(val_result_path, exist_ok=True)

                for current_step, val_data in enumerate(val_loader):
                    with torch.no_grad():  # éªŒè¯æ—¶ä¸éœ€è¦æ¢¯åº¦
                        process_batch_efficiently(val_data, diffusion, change_detection, opt, "val")
                        change_detection.test()
                        change_detection._collect_running_batch_states()
                    
                    # éªŒè¯æ—¥å¿—ï¼ˆå‡å°‘é¢‘ç‡ï¼‰
                    optimized_logging(current_step, current_epoch, n_epoch, val_loader, 
                                    change_detection, logger, opt, "val")
                    
                    # éªŒè¯å¯è§†åŒ–ï¼ˆæ›´å°‘é¢‘ç‡ï¼‰
                    if current_step % max(1, len(val_loader) // 3) == 0:
                        try:
                            visuals = change_detection.get_current_visuals()
                            device = val_data['A'].device
                            visuals['pred_cm'] = visuals['pred_cm'] * 2.0 - 1.0
                            visuals['gt_cm'] = visuals['gt_cm'] * 2.0 - 1.0
                            
                            pred_cm_expanded = visuals['pred_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
                            gt_cm_expanded = visuals['gt_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
                            
                            grid_img = torch.cat((val_data['A'], val_data['B'], 
                                                pred_cm_expanded, gt_cm_expanded), dim=0)
                            grid_img = Metrics.tensor2img(grid_img)
                            
                            Metrics.save_img(grid_img, 
                                f'{val_result_path}/img_A_B_pred_gt_e{current_epoch}_b{current_step}.png')
                        except Exception as e:
                            print(f"éªŒè¯å¯è§†åŒ–å¤±è´¥: {e}")

                ### éªŒè¯æ€»ç»“ ### 
                try:
                    change_detection._collect_epoch_states()
                    logs = change_detection.get_current_log()
                    
                    val_time = time.time() - val_start_time
                    message = f'[éªŒè¯ Epoch {current_epoch}] mF1={logs["epoch_acc"]:.5f}, ' \
                            f'ç”¨æ—¶={val_time/60:.1f}åˆ†é’Ÿ'
                    
                    print(f"âœ… {message}")
                    logger.info(message)
                    
                    for k, v in logs.items():
                        tb_logger.add_scalar(f'val/{k}', v, current_epoch)

                    # ğŸ” è¯¦ç»†çš„WandBè°ƒè¯•è®°å½•
                    if wandb_logger:
                        try:
                            # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰logs
                            print("\nğŸ” === WandBè°ƒè¯•ä¿¡æ¯ ===")
                            print(f"å½“å‰epoch: {current_epoch}")
                            print(f"best_mF1: {best_mF1} (ç±»å‹: {type(best_mF1)})")
                            print("logså†…å®¹:")
                            for k, v in logs.items():
                                print(f"  {k}: {v} (ç±»å‹: {type(v)})")
                            
                            # å®‰å…¨è½¬æ¢æ‰€æœ‰æŒ‡æ ‡
                            def safe_convert(value, key):
                                if value is None:
                                    print(f"  âš ï¸  {key}: Noneå€¼")
                                    return None
                                try:
                                    if hasattr(value, 'item'):  # PyTorch tensor
                                        result = float(value.item())
                                    else:
                                        result = float(value)
                                    
                                    # æ£€æŸ¥NaNå’Œæ— ç©·å¤§
                                    if result != result or result == float('inf') or result == float('-inf'):
                                        print(f"  âŒ {key}: æ— æ•ˆæ•°å€¼ {result}")
                                        return None
                                    
                                    print(f"  âœ… {key}: {value} â†’ {result}")
                                    return result
                                except Exception as e:
                                    print(f"  âŒ {key}: è½¬æ¢å¤±è´¥ {value} - {e}")
                                    return None
                            
                            # æ„å»ºå®‰å…¨çš„æŒ‡æ ‡å­—å…¸
                            validation_metrics = {}
                            
                            # ä¸»è¦æŒ‡æ ‡
                            for wandb_key, log_key in [
                                ('validation/mF1', 'epoch_acc'),
                                ('validation/loss', 'l_cd'),
                                ('validation/mIoU', 'miou'),
                                ('validation/accuracy', 'acc'),
                                ('validation/change_F1', 'F1_1'),
                                ('validation/no_change_F1', 'F1_0'),
                                ('validation/change_IoU', 'iou_1'),
                                ('validation/no_change_IoU', 'iou_0'),
                            ]:
                                converted = safe_convert(logs.get(log_key), wandb_key)
                                if converted is not None:
                                    validation_metrics[wandb_key] = converted
                            
                            # ç®€åŒ–å‘½åçš„æŒ‡æ ‡
                            for wandb_key, log_key in [
                                ('val_mF1', 'epoch_acc'),
                                ('val_loss', 'l_cd'),
                                ('val_mIoU', 'miou'),
                                ('val_accuracy', 'acc'),
                            ]:
                                converted = safe_convert(logs.get(log_key), wandb_key)
                                if converted is not None:
                                    validation_metrics[wandb_key] = converted
                            
                            # å…¶ä»–æŒ‡æ ‡
                            validation_metrics['epoch'] = current_epoch
                            validation_metrics['validation_step'] = current_epoch
                            
                            # best_mF1
                            converted_best = safe_convert(best_mF1, 'val_best_mF1')
                            if converted_best is not None:
                                validation_metrics['val_best_mF1'] = converted_best
                                validation_metrics['validation/best_mF1'] = converted_best
                            
                            # æ—¶é—´
                            validation_metrics['validation/time_minutes'] = val_time / 60
                            
                            print(f"\nğŸ“Š å°†è¦è®°å½•çš„æŒ‡æ ‡ ({len(validation_metrics)}ä¸ª):")
                            for k, v in validation_metrics.items():
                                print(f"  {k}: {v}")
                            
                            # è®°å½•åˆ°WandB
                            if validation_metrics:
                                wandb_logger.log_metrics(validation_metrics)
                                print(f"\nâœ… WandBè®°å½•æˆåŠŸ: {len(validation_metrics)}ä¸ªæŒ‡æ ‡")
                            else:
                                print("\nâŒ æ²¡æœ‰æœ‰æ•ˆæŒ‡æ ‡å¯è®°å½•")
                            
                            print("ğŸ” === WandBè°ƒè¯•ä¿¡æ¯ç»“æŸ ===\n")
                            
                        except Exception as e:
                            print(f"âŒ WandBè®°å½•é”™è¯¯: {e}")
                            import traceback
                            traceback.print_exc()
                    
                    # æ¨¡å‹ä¿å­˜é€»è¾‘ä¿æŒä¸å˜
                    if logs['epoch_acc'] > best_mF1:
                        is_best_model = True
                        best_mF1 = logs['epoch_acc']
                        print(f"ğŸ‰ æœ€ä½³æ¨¡å‹æ›´æ–°! mF1: {best_mF1:.5f}")
                        logger.info('[éªŒè¯] æœ€ä½³æ¨¡å‹æ›´æ–°ï¼Œä¿å­˜æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€')
                    else:
                        is_best_model = False
                        logger.info('[éªŒè¯] ä¿å­˜å½“å‰æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€')

                    change_detection.save_network(current_epoch, is_best_model=is_best_model)
                    
                except Exception as e:
                    print(f"éªŒè¯æŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
                
                change_detection._clear_cache()
                print(f"--- è¿›å…¥ä¸‹ä¸€ä¸ªEpoch ---\n")

            if wandb_logger:
                wandb_logger.log_metrics({'epoch': current_epoch})
            
            # Epochç»“æŸæ¸…ç†
            torch.cuda.empty_cache()
                
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        logger.info('è®­ç»ƒç»“æŸ')
        
    else:
        ##################
        ### æµ‹è¯•é˜¶æ®µ ###
        ##################
        logger.info('å¼€å§‹æ¨¡å‹è¯„ä¼°ï¼ˆæµ‹è¯•ï¼‰')
        print("ğŸ” å¼€å§‹æµ‹è¯•...")
        
        test_result_path = f'{opt["path"]["results"]}/test/'
        os.makedirs(test_result_path, exist_ok=True)
        logger_test = logging.getLogger('test')
        change_detection._clear_cache()
        
        for current_step, test_data in enumerate(test_loader):
            with torch.no_grad():
                process_batch_efficiently(test_data, diffusion, change_detection, opt, "test")
                change_detection.test()
                change_detection._collect_running_batch_states()

            # æµ‹è¯•æ—¥å¿—
            if current_step % max(1, len(test_loader) // 10) == 0:
                logs = change_detection.get_current_log()
                message = f'[æµ‹è¯•] Step {current_step}/{len(test_loader)}, ' \
                         f'mF1: {logs["running_acc"]:.5f}'
                print(message)
                logger_test.info(message)

            # ä¿å­˜æµ‹è¯•ç»“æœ
            try:
                visuals = change_detection.get_current_visuals()
                visuals['pred_cm'] = visuals['pred_cm'] * 2.0 - 1.0
                visuals['gt_cm'] = visuals['gt_cm'] * 2.0 - 1.0
                
                # å•ç‹¬ä¿å­˜å›¾åƒ
                img_A = Metrics.tensor2img(test_data['A'], out_type=np.uint8, min_max=(-1, 1))
                img_B = Metrics.tensor2img(test_data['B'], out_type=np.uint8, min_max=(-1, 1))
                gt_cm = Metrics.tensor2img(visuals['gt_cm'].unsqueeze(1).repeat(1, 3, 1, 1), 
                                          out_type=np.uint8, min_max=(0, 1))
                pred_cm = Metrics.tensor2img(visuals['pred_cm'].unsqueeze(1).repeat(1, 3, 1, 1), 
                                            out_type=np.uint8, min_max=(0, 1))

                Metrics.save_img(img_A, f'{test_result_path}/img_A_{current_step}.png')
                Metrics.save_img(img_B, f'{test_result_path}/img_B_{current_step}.png')
                Metrics.save_img(pred_cm, f'{test_result_path}/img_pred_cm{current_step}.png')
                Metrics.save_img(gt_cm, f'{test_result_path}/img_gt_cm{current_step}.png')
                
            except Exception as e:
                print(f"æµ‹è¯•ä¿å­˜å¤±è´¥: {e}")

        ### æµ‹è¯•æ€»ç»“ ###
        try:
            change_detection._collect_epoch_states()
            logs = change_detection.get_current_log()
            
            message = f'[æµ‹è¯•æ€»ç»“] mF1={logs["epoch_acc"]:.5f}\n'
            for k, v in logs.items():
                message += f'{k}: {v:.4e} '
            message += '\n'
            
            print(f"âœ… {message}")
            logger_test.info(message)

            if wandb_logger:
                wandb_logger.log_metrics({
                    'test/mF1': logs['epoch_acc'],
                    'test/mIoU': logs['miou'],
                    'test/OA': logs['acc'],
                    'test/change-F1': logs['F1_1'],
                    'test/no-change-F1': logs['F1_0'],
                    'test/change-IoU': logs['iou_1'],
                    'test/no-change-IoU': logs['iou_0'],
                })
                
        except Exception as e:
            print(f"æµ‹è¯•æŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")

        print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
        logger.info('æµ‹è¯•ç»“æŸ')
        
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒ
# import torch.multiprocessing as mp  # å¤šè¿›ç¨‹
# from torch.nn.parallel import DistributedDataParallel as DDP  # åˆ†å¸ƒå¼DataParallel
# from torch.utils.data.distributed import DistributedSampler  # åˆ†å¸ƒå¼é‡‡æ ·å™¨

# import data as Data
# import model as Model
# import argparse
# import logging
# import core.logger as Logger
# import core.metrics as Metrics
# from core.wandb_logger import WandbLogger
# from tensorboardX import SummaryWriter
# import os
# import numpy as np
# from model.cd_modules.cd_head import cd_head 
# from misc.print_diffuse_feats import print_feats
# import time
# from contextlib import contextmanager

# import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'  # ä½¿ç”¨4ä¸ªGPU
# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥CUDAè°ƒç”¨ï¼Œä¾¿äºè°ƒè¯•

# # ==================== ä¼˜åŒ–ç‰ˆæ ‡ç­¾éªŒè¯å™¨ ====================
# class LabelValidator:
#     """é«˜æ•ˆæ ‡ç­¾éªŒè¯å™¨ - å•ä¾‹æ¨¡å¼"""
#     _instance = None
#     _initialized = False
    
#     def __new__(cls):
#         if cls._instance is None:
#             cls._instance = super().__new__(cls)
#         return cls._instance
    
#     def __init__(self):
#         if not self._initialized:
#             self.is_normalized = None
#             self.validation_done = False
#             self.label_stats = {}
#             LabelValidator._initialized = True
    
#     def validate_and_fix_labels(self, data, phase="train"):
#         """
#         é«˜æ•ˆæ ‡ç­¾éªŒè¯ - åªåœ¨ç¬¬ä¸€æ¬¡è¯¦ç»†æ£€æŸ¥ï¼Œåç»­å¿«é€Ÿå¤„ç†
#         """
#         if 'L' not in data:
#             return False
        
#         labels = data['L']
        
#         # å¿«é€Ÿé€šé“ï¼šå¦‚æœå·²ç»éªŒè¯è¿‡ï¼Œç›´æ¥å¤„ç†
#         if self.validation_done:
#             if self.is_normalized:
#                 data['L'] = (labels >= 0.5).long()
#             else:
#                 fixed_labels = labels.clone()
#                 unique_vals = torch.unique(labels)
#                 if 255 in unique_vals:
#                     fixed_labels[labels == 255] = 1
#                 data['L'] = torch.clamp(fixed_labels, 0, 1).long()
#             return True
        
#         # ç¬¬ä¸€æ¬¡è¯¦ç»†éªŒè¯
#         unique_vals = torch.unique(labels)
#         min_val = labels.min().item()
#         max_val = labels.max().item()
        
#         print(f"\nğŸ” [{phase}] æ ‡ç­¾éªŒè¯ï¼ˆä»…æ˜¾ç¤ºä¸€æ¬¡ï¼‰:")
#         print(f"   å½¢çŠ¶: {labels.shape}, æ•°æ®ç±»å‹: {labels.dtype}")
#         print(f"   å€¼èŒƒå›´: [{min_val}, {max_val}]")
#         print(f"   å”¯ä¸€å€¼: {unique_vals.tolist()}")
        
#         # åˆ¤æ–­æ ‡ç­¾ç±»å‹
#         self.is_normalized = (max_val <= 1.0 and min_val >= 0.0)
        
#         if self.is_normalized:
#             print(f"   ğŸ”§ æ£€æµ‹åˆ°å½’ä¸€åŒ–æ ‡ç­¾ï¼Œä½¿ç”¨é˜ˆå€¼äºŒå€¼åŒ–ï¼ˆé˜ˆå€¼=0.5ï¼‰")
#             fixed_labels = (labels >= 0.5).long()
#         else:
#             print(f"   ğŸ”§ æ£€æµ‹åˆ°æ ‡å‡†æ ‡ç­¾ï¼Œæ˜ å°„255â†’1")
#             fixed_labels = labels.clone()
#             if 255 in unique_vals:
#                 fixed_labels[labels == 255] = 1
#             fixed_labels = torch.clamp(fixed_labels, 0, 1).long()
        
#         # éªŒè¯ä¿®å¤ç»“æœ
#         final_unique = torch.unique(fixed_labels)
#         zero_count = (fixed_labels == 0).sum().item()
#         one_count = (fixed_labels == 1).sum().item()
#         total = zero_count + one_count
        
#         print(f"   âœ… ä¿®å¤å®Œæˆ: å”¯ä¸€å€¼{final_unique.tolist()}")
#         print(f"   ğŸ“Š åƒç´ åˆ†å¸ƒ: æ— å˜åŒ–={100*zero_count/total:.1f}%, æœ‰å˜åŒ–={100*one_count/total:.1f}%")
#         print(f"   âœ… æ ‡ç­¾éªŒè¯è®¾ç½®å®Œæˆï¼Œåç»­æ‰¹æ¬¡å°†å¿«é€Ÿå¤„ç†\n")
        
#         # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
#         self.label_stats = {
#             'zero_ratio': zero_count / total,
#             'one_ratio': one_count / total,
#             'is_normalized': self.is_normalized
#         }
        
#         data['L'] = fixed_labels
#         self.validation_done = True
#         return True

# # å…¨å±€æ ‡ç­¾éªŒè¯å™¨
# label_validator = LabelValidator()

# # ==================== å†…å­˜ç®¡ç†å·¥å…· ====================
# @contextmanager
# def memory_efficient_context():
#     """å†…å­˜ç®¡ç†ä¸Šä¸‹æ–‡"""
#     if torch.cuda.is_available():
#         torch.cuda.empty_cache()
#     try:
#         yield
#     finally:
#         if torch.cuda.is_available():
#             torch.cuda.empty_cache()

# class PerformanceMonitor:
#     """æ€§èƒ½ç›‘æ§å™¨"""
#     def __init__(self):
#         self.step_times = []
#         self.memory_usage = []
#         self.start_time = time.time()
    
#     def log_step(self, step_time, memory_mb=None):
#         self.step_times.append(step_time)
#         if memory_mb:
#             self.memory_usage.append(memory_mb)
    
#     def get_stats(self):
#         if not self.step_times:
#             return "æ— ç»Ÿè®¡æ•°æ®"
        
#         avg_time = np.mean(self.step_times[-100:])  # æœ€è¿‘100æ­¥å¹³å‡
#         total_time = time.time() - self.start_time
        
#         stats = f"å¹³å‡æ­¥æ—¶: {avg_time:.2f}s, æ€»æ—¶é—´: {total_time/60:.1f}min"
        
#         if self.memory_usage:
#             avg_memory = np.mean(self.memory_usage[-10:])
#             stats += f", æ˜¾å­˜: {avg_memory:.1f}MB"
        
#         return stats

# # ==================== ä¼˜åŒ–ç‰ˆç‰¹å¾é‡æ’ ====================
# def apply_feature_reordering_optimized(f_A, f_B, train_data, change_detection, opt, phase="train"):
#     """
#     å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾é‡æ’æ–¹æ¡ˆ
#     """
#     try:
#         feat_scales = opt['model_cd']['feat_scales']  # [2, 5, 8, 11, 14]
#         cd_expected_order = sorted(feat_scales, reverse=True)  # [14, 11, 8, 5, 2]
        
#         # åªåœ¨ç¬¬ä¸€æ¬¡æ˜¾ç¤ºä¿¡æ¯
#         if not hasattr(apply_feature_reordering_optimized, '_logged'):
#             print("ğŸ¯ ä½¿ç”¨ä¼˜åŒ–çš„ç‰¹å¾é‡æ’æ–¹æ¡ˆ")
#             print("   ä¿æŒåŸå§‹å¤šå°ºåº¦é…ç½®çš„å®Œæ•´è¯­ä¹‰")
#             for i, scale in enumerate(cd_expected_order):
#                 print(f"     Block{i}: ä½¿ç”¨layer{scale}ç‰¹å¾")
#             apply_feature_reordering_optimized._logged = True
        
#         # é«˜æ•ˆé‡æ’ï¼šç›´æ¥åœ¨åŸåœ°ä¿®æ”¹
#         reordered_f_A = []
#         reordered_f_B = []
        
#         for fa, fb in zip(f_A, f_B):
#             if isinstance(fa, list) and len(fa) > max(feat_scales):
#                 timestep_A = [fa[scale] for scale in cd_expected_order]
#                 timestep_B = [fb[scale] for scale in cd_expected_order]
#                 reordered_f_A.append(timestep_A)
#                 reordered_f_B.append(timestep_B)
#             else:
#                 raise ValueError(f"ç‰¹å¾æ ¼å¼é”™è¯¯: æœŸæœ›listé•¿åº¦>{max(feat_scales)}, å®é™…{type(fa)}")
        
#         # æ¸…ç†åŸå§‹ç‰¹å¾é‡Šæ”¾å†…å­˜
#         del f_A, f_B
        
#         # ä½¿ç”¨é‡æ’åçš„ç‰¹å¾
#         change_detection.feed_data(reordered_f_A, reordered_f_B, train_data)
        
#         # æ¸…ç†é‡æ’åçš„ç‰¹å¾
#         del reordered_f_A, reordered_f_B
        
#         return True
        
#     except Exception as e:
#         print(f"âŒ ç‰¹å¾é‡æ’å¤±è´¥: {e}")
#         print("ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ¡ˆ...")
        
#         # ç®€åŒ–å›é€€æ–¹æ¡ˆ
#         target_layers = [12, 13, 14]
#         corrected_f_A = []
#         corrected_f_B = []
        
#         for fa, fb in zip(f_A, f_B):
#             timestep_A = [fa[i] for i in target_layers if i < len(fa)]
#             timestep_B = [fb[i] for i in target_layers if i < len(fb)]
#             corrected_f_A.append(timestep_A)
#             corrected_f_B.append(timestep_B)
        
#         del f_A, f_B
#         change_detection.feed_data(corrected_f_A, corrected_f_B, train_data)
#         del corrected_f_A, corrected_f_B
        
#         return False

# # ==================== è®­ç»ƒä¼˜åŒ–è®¾ç½® ====================
# def setup_training_optimization(diffusion, change_detection):
#     """è®¾ç½®è®­ç»ƒä¼˜åŒ–"""
#     print("ğŸš€ è®¾ç½®è®­ç»ƒä¼˜åŒ–...")
    
#     # å¯ç”¨CUDAä¼˜åŒ–
#     torch.backends.cudnn.benchmark = True
#     torch.backends.cudnn.deterministic = False
    
#     # æ£€æŸ¥æ··åˆç²¾åº¦æ”¯æŒ
#     use_amp = False
#     if torch.cuda.is_available():
#         try:
#             from torch.cuda.amp import autocast, GradScaler
#             use_amp = True
#             print("   âœ… æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ")
#         except ImportError:
#             print("   âš ï¸  ä¸æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ")
    
#     # è®¾ç½®diffusionæ¨¡å‹ä¸ºevalæ¨¡å¼ï¼ˆå¦‚æœä¸éœ€è¦è®­ç»ƒï¼‰
#     if hasattr(diffusion.netG, 'eval'):
#         diffusion.netG.eval()
#         print("   âœ… Diffusionæ¨¡å‹è®¾ç½®ä¸ºevalæ¨¡å¼")
    
#     # æ£€æŸ¥å¤šGPUè®¾ç½®
#     if torch.cuda.device_count() > 1:
#         print(f"   âœ… æ£€æµ‹åˆ°{torch.cuda.device_count()}ä¸ªGPU")
        
#         # æ˜¾ç¤ºGPUçŠ¶æ€
#         for i in range(torch.cuda.device_count()):
#             props = torch.cuda.get_device_properties(i)
#             memory_gb = props.total_memory / 1024**3
#             print(f"     GPU {i}: {props.name} ({memory_gb:.1f}GB)")
    
#     print("ğŸš€ è®­ç»ƒä¼˜åŒ–è®¾ç½®å®Œæˆ\n")
    
#     return use_amp

# # ==================== æ‰¹é‡å¤„ç†ä¼˜åŒ– ====================
# def process_batch_efficiently(train_data, diffusion, change_detection, opt, phase="train"):
#     """é«˜æ•ˆçš„æ‰¹é‡å¤„ç†"""
#     with memory_efficient_context():
#         # 1. å¿«é€Ÿæ ‡ç­¾éªŒè¯
#         label_validator.validate_and_fix_labels(train_data, phase)
        
#         # 2. ç‰¹å¾æå–
#         diffusion.feed_data(train_data)
        
#         # 3. æ”¶é›†ç‰¹å¾
#         f_A, f_B = [], []
#         for t in opt['model_cd']['t']:
#             fe_A_t, fd_A_t, fe_B_t, fd_B_t = diffusion.get_feats(t=t)
#             if opt['model_cd']['feat_type'] == "dec":
#                 f_A.append(fd_A_t)
#                 f_B.append(fd_B_t)
#                 del fe_A_t, fe_B_t  # ç«‹å³æ¸…ç†
#             else:
#                 f_A.append(fe_A_t)
#                 f_B.append(fe_B_t)
#                 del fd_A_t, fd_B_t  # ç«‹å³æ¸…ç†
        
#         # 4. ç‰¹å¾é‡æ’
#         apply_feature_reordering_optimized(f_A, f_B, train_data, change_detection, opt, phase)

# # ==================== ä¼˜åŒ–çš„æ—¥å¿—ç®¡ç† ====================
# def optimized_logging(current_step, current_epoch, n_epoch, loader, change_detection, 
#                      logger, opt, phase="train", performance_monitor=None):
#     """ä¼˜åŒ–çš„æ—¥å¿—è¾“å‡º"""
#     # åŠ¨æ€è°ƒæ•´æ—¥å¿—é¢‘ç‡
#     if phase == "train":
#         base_freq = opt['train'].get('train_print_freq', 10)
#         log_freq = max(base_freq, len(loader) // 1000)  # è‡³å°‘æ¯5%æ˜¾ç¤ºä¸€æ¬¡
#     else:
#         log_freq = max(1, len(loader) // 500)  # éªŒè¯æ—¶æ¯10%æ˜¾ç¤ºä¸€æ¬¡
    
#     if current_step % log_freq == 0:
#         try:
#             logs = change_detection.get_current_log()
            
#             # åŸºç¡€ä¿¡æ¯
#             progress = f"[{current_epoch}/{n_epoch-1}]"
#             step_info = f"Step {current_step}/{len(loader)}"
#             metrics = f"Loss: {logs.get('l_cd', 0):.5f} mF1: {logs.get('running_acc', 0):.5f}"
            
#             # æ€§èƒ½ä¿¡æ¯
#             perf_info = ""
#             if performance_monitor:
#                 perf_info = f" | {performance_monitor.get_stats()}"
            
#             message = f"{progress} {step_info} {metrics}{perf_info}"
#             print(message)
            
#         except Exception as e:
#             print(f"æ—¥å¿—è¾“å‡ºé”™è¯¯: {e}")

# # ==================== é”™è¯¯å¤„ç†è£…é¥°å™¨ ====================
# def safe_training_step(func):
#     """å®‰å…¨è®­ç»ƒæ­¥éª¤è£…é¥°å™¨"""
#     def wrapper(*args, **kwargs):
#         try:
#             return func(*args, **kwargs)
#         except RuntimeError as e:
#             if "device-side assert triggered" in str(e) or "CUDA" in str(e):
#                 print(f"âš ï¸  CUDAé”™è¯¯å·²è‡ªåŠ¨å¤„ç†: {str(e)[:100]}...")
#                 torch.cuda.empty_cache()
#                 return False
#             else:
#                 raise
#         except Exception as e:
#             print(f"âŒ è®­ç»ƒæ­¥éª¤é”™è¯¯: {e}")
#             return False
#     return wrapper

# @safe_training_step
# def execute_training_step(change_detection):
#     """æ‰§è¡Œè®­ç»ƒæ­¥éª¤"""
#     change_detection.optimize_parameters()
#     change_detection._collect_running_batch_states()
#     return True

# # ==================== checkpointæ¢å¤ ====================
# def load_checkpoint_if_exists(change_detection, opt):
#     """ä¿®æ­£ç‰ˆæœ¬ï¼šä¼˜å…ˆbestæ¨¡å‹ï¼Œä¿®å¤åŠ è½½æ¥å£"""
    
#     # ğŸ¯ æŒ‡å®šçš„checkpointè·¯å¾„
#     checkpoint_dir = os.path.expanduser("~/bowen/DDPM-CD/ddpm-cd/checkpoint/gvlm_cd_ddpm")
    
#     if not os.path.exists(checkpoint_dir):
#         print(f"ğŸ” Checkpointç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
#         return 0, 0.0
    
#     print(f"ğŸ” æ£€æŸ¥checkpointç›®å½•: {checkpoint_dir}")
    
#     import glob
#     import re
    
#     # ========================================
#     # ğŸ¥‡ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæ£€æŸ¥æœ€ä½³æ¨¡å‹
#     # ========================================
#     best_gen_file = os.path.join(checkpoint_dir, "best_cd_model_gen.pth")
#     best_opt_file = os.path.join(checkpoint_dir, "best_cd_model_opt.pth")
    
#     if os.path.exists(best_gen_file):
#         print("ğŸ† å‘ç°æœ€ä½³æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨æœ€ä½³æ¨¡å‹")
#         print(f"   æœ€ä½³æ¨¡å‹æ–‡ä»¶: {best_gen_file}")
#         print(f"   æœ€ä½³ä¼˜åŒ–å™¨æ–‡ä»¶: {best_opt_file}")
        
#         success = load_model_safe(change_detection, best_gen_file, best_opt_file)
        
#         if success:
#             print("âœ… æœ€ä½³æ¨¡å‹åŠ è½½æˆåŠŸ")
#             # ä»æœ€ä½³æ¨¡å‹å¼€å§‹ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ªè¾ƒé«˜çš„epochæˆ–ä»0å¼€å§‹
#             return 0, 0.8  # ä»epoch 0å¼€å§‹ï¼Œä½†best_mF1è®¾ç½®è¾ƒé«˜å€¼è¡¨ç¤ºè¿™æ˜¯å¥½æ¨¡å‹
#         else:
#             print("âŒ æœ€ä½³æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•æœ€æ–°epochæ¨¡å‹")
    
#     # ========================================
#     # ğŸ¥ˆ ç¬¬äºŒä¼˜å…ˆçº§ï¼šæŸ¥æ‰¾æœ€æ–°epochæ¨¡å‹
#     # ========================================
#     gen_files = glob.glob(os.path.join(checkpoint_dir, "cd_model_E*_gen.pth"))
    
#     if gen_files:
#         print(f"ğŸ” æ‰¾åˆ°çš„epochæ¨¡å‹æ–‡ä»¶: {[os.path.basename(f) for f in gen_files]}")
        
#         # æå–epochæ•°å­—å¹¶æ’åº
#         epochs = []
#         for f in gen_files:
#             match = re.search(r'cd_model_E(\d+)_gen\.pth', f)
#             if match:
#                 epochs.append(int(match.group(1)))
        
#         if epochs:
#             latest_epoch = max(epochs)
            
#             # æ„å»ºæ–‡ä»¶è·¯å¾„
#             gen_file = os.path.join(checkpoint_dir, f"cd_model_E{latest_epoch}_gen.pth")
#             opt_file = os.path.join(checkpoint_dir, f"cd_model_E{latest_epoch}_opt.pth")
            
#             print(f"ğŸ”„ ä½¿ç”¨æœ€æ–°epochæ¨¡å‹: Epoch {latest_epoch}")
#             print(f"   æ¨¡å‹æ–‡ä»¶: {gen_file}")
#             print(f"   ä¼˜åŒ–å™¨æ–‡ä»¶: {opt_file}")
            
#             success = load_model_safe(change_detection, gen_file, opt_file)
            
#             if success:
#                 print("âœ… æœ€æ–°epochæ¨¡å‹åŠ è½½æˆåŠŸ")
                
#                 # æ£€æŸ¥best_mF1ï¼ˆå¯ä»¥ä»æŸä¸ªè®°å½•æ–‡ä»¶è¯»å–ï¼Œæˆ–è®¾ç½®é»˜è®¤å€¼ï¼‰
#                 best_mF1 = 0.0
#                 if os.path.exists(best_gen_file):
#                     best_mF1 = 0.5  # å¦‚æœæœ‰bestæ–‡ä»¶ä½†åŠ è½½å¤±è´¥ï¼Œè®¾ç½®ä¸€ä¸ªä¸­ç­‰å€¼
                
#                 return latest_epoch + 1, best_mF1
#             else:
#                 print("âŒ æœ€æ–°epochæ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥")
    
#     print("ğŸ†• æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
#     return 0, 0.0


# def load_model_safe(change_detection, gen_file, opt_file):
#     """å®‰å…¨çš„æ¨¡å‹åŠ è½½æ–¹æ³• - å°è¯•å¤šç§åŠ è½½æ–¹å¼"""
    
#     if not os.path.exists(gen_file):
#         print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {gen_file}")
#         return False
    
#     print(f"ğŸ”„ å°è¯•åŠ è½½æ¨¡å‹: {os.path.basename(gen_file)}")
    
#     # ========================================
#     # æ–¹æ³•1: ç›´æ¥torch.load + æ‰‹åŠ¨è®¾ç½®state_dict
#     # ========================================
#     try:
#         print("   ğŸ”„ æ–¹æ³•1: ç›´æ¥torch.load")
#         checkpoint = torch.load(gen_file, map_location='cpu')
        
#         if hasattr(change_detection, 'netCD') and change_detection.netCD is not None:
#             # æ£€æŸ¥checkpointç»“æ„
#             if isinstance(checkpoint, dict):
#                 print(f"   ğŸ“‹ Checkpoint keys: {list(checkpoint.keys())}")
                
#                 # å°è¯•ä¸åŒçš„key
#                 state_dict = None
#                 for key in ['model_state_dict', 'state_dict', 'model', 'netCD']:
#                     if key in checkpoint:
#                         state_dict = checkpoint[key]
#                         print(f"   âœ… ä½¿ç”¨key: {key}")
#                         break
                
#                 if state_dict is None:
#                     # ç›´æ¥ä½œä¸ºstate_dict
#                     state_dict = checkpoint
#                     print("   âœ… ç›´æ¥ä½œä¸ºstate_dict")
#             else:
#                 state_dict = checkpoint
#                 print("   âœ… Checkpointæ˜¯state_dict")
            
#             # åŠ è½½state_dict
#             change_detection.netCD.load_state_dict(state_dict, strict=False)
#             print("   âœ… æ–¹æ³•1: æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
            
#             # å°è¯•åŠ è½½ä¼˜åŒ–å™¨
#             load_optimizer_safe(change_detection, opt_file)
            
#             return True
            
#     except Exception as e:
#         print(f"   âŒ æ–¹æ³•1å¤±è´¥: {e}")
    
#     # ========================================
#     # æ–¹æ³•2: å°è¯•æ— å‚æ•°load_network (è®¾ç½®è·¯å¾„)
#     # ========================================
#     try:
#         print("   ğŸ”„ æ–¹æ³•2: æ— å‚æ•°load_network")
        
#         # å°è¯•è®¾ç½®è·¯å¾„åˆ°optä¸­
#         if hasattr(change_detection, 'opt'):
#             # å¤‡ä»½åŸè·¯å¾„
#             original_path = change_detection.opt.get('path', {}).get('resume_state', '')
            
#             # è®¾ç½®æ–°è·¯å¾„
#             if 'path' not in change_detection.opt:
#                 change_detection.opt['path'] = {}
#             change_detection.opt['path']['resume_state'] = gen_file
            
#             # å°è¯•åŠ è½½
#             change_detection.load_network()
            
#             # æ¢å¤åŸè·¯å¾„
#             if original_path:
#                 change_detection.opt['path']['resume_state'] = original_path
            
#             print("   âœ… æ–¹æ³•2: åŠ è½½æˆåŠŸ")
#             load_optimizer_safe(change_detection, opt_file)
#             return True
            
#     except Exception as e:
#         print(f"   âŒ æ–¹æ³•2å¤±è´¥: {e}")
    
#     # ========================================
#     # æ–¹æ³•3: æŸ¥æ‰¾å…¶ä»–åŠ è½½æ–¹æ³•
#     # ========================================
#     try:
#         print("   ğŸ”„ æ–¹æ³•3: æŸ¥æ‰¾å…¶ä»–åŠ è½½æ–¹æ³•")
        
#         # å°è¯•å¸¸è§çš„æ–¹æ³•å
#         load_methods = ['load_model', 'load_checkpoint', 'resume_from_checkpoint', 'load_state_dict']
        
#         for method_name in load_methods:
#             if hasattr(change_detection, method_name):
#                 print(f"   ğŸ”„ å°è¯•: {method_name}")
#                 method = getattr(change_detection, method_name)
                
#                 try:
#                     # å°è¯•å¸¦å‚æ•°è°ƒç”¨
#                     method(gen_file)
#                     print(f"   âœ… æ–¹æ³•3æˆåŠŸ: {method_name}(gen_file)")
#                     load_optimizer_safe(change_detection, opt_file)
#                     return True
#                 except TypeError:
#                     # å°è¯•æ— å‚æ•°è°ƒç”¨
#                     try:
#                         method()
#                         print(f"   âœ… æ–¹æ³•3æˆåŠŸ: {method_name}()")
#                         load_optimizer_safe(change_detection, opt_file)
#                         return True
#                     except:
#                         continue
#                 except Exception as e:
#                     print(f"   âŒ {method_name}å¤±è´¥: {e}")
#                     continue
        
#     except Exception as e:
#         print(f"   âŒ æ–¹æ³•3å¤±è´¥: {e}")
    
#     print("   âŒ æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥")
#     return False


# def load_optimizer_safe(change_detection, opt_file):
#     """å®‰å…¨çš„ä¼˜åŒ–å™¨åŠ è½½"""
#     if not os.path.exists(opt_file):
#         print("   âš ï¸  ä¼˜åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨")
#         return False
    
#     try:
#         print(f"   ğŸ”„ åŠ è½½ä¼˜åŒ–å™¨: {os.path.basename(opt_file)}")
#         opt_state = torch.load(opt_file, map_location='cpu')
        
#         # æ£€æŸ¥ä¼˜åŒ–å™¨å±æ€§
#         optimizer = None
#         for attr_name in ['optCD', 'optimizer', 'opt_CD', 'optim']:
#             if hasattr(change_detection, attr_name):
#                 optimizer = getattr(change_detection, attr_name)
#                 if optimizer is not None:
#                     print(f"   ğŸ“‹ æ‰¾åˆ°ä¼˜åŒ–å™¨å±æ€§: {attr_name}")
#                     break
        
#         if optimizer is not None:
#             # æ£€æŸ¥opt_stateç»“æ„
#             if isinstance(opt_state, dict) and 'state_dict' in opt_state:
#                 optimizer.load_state_dict(opt_state['state_dict'])
#             else:
#                 optimizer.load_state_dict(opt_state)
            
#             print("   âœ… ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
#             return True
#         else:
#             print("   âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä¼˜åŒ–å™¨å±æ€§")
#             return False
            
#     except Exception as e:
#         print(f"   âŒ ä¼˜åŒ–å™¨åŠ è½½å¤±è´¥: {e}")
#         return False


# def debug_change_detection_methods(change_detection):
#     """è°ƒè¯•change_detectionçš„æ–¹æ³• - å¯é€‰è°ƒç”¨"""
#     print("\nğŸ” === change_detection è°ƒè¯•ä¿¡æ¯ ===")
    
#     # æŸ¥çœ‹ç±»å‹
#     print(f"å¯¹è±¡ç±»å‹: {type(change_detection)}")
    
#     # æŸ¥çœ‹loadç›¸å…³æ–¹æ³•
#     load_methods = [attr for attr in dir(change_detection) if 'load' in attr.lower() and callable(getattr(change_detection, attr))]
#     print(f"Loadæ–¹æ³•: {load_methods}")
    
#     # æŸ¥çœ‹ä¼˜åŒ–å™¨ç›¸å…³å±æ€§
#     opt_attrs = [attr for attr in dir(change_detection) if 'opt' in attr.lower()]
#     print(f"ä¼˜åŒ–å™¨ç›¸å…³å±æ€§: {opt_attrs}")
    
#     # æŸ¥çœ‹ç½‘ç»œç›¸å…³å±æ€§
#     net_attrs = [attr for attr in dir(change_detection) if 'net' in attr.lower()]
#     print(f"ç½‘ç»œç›¸å…³å±æ€§: {net_attrs}")
    
#     print("ğŸ” === è°ƒè¯•ä¿¡æ¯ç»“æŸ ===\n")

# # ==================== ä¸»å‡½æ•° ====================
# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument('-c', '--config', type=str, default='config/ddpm_cd.json',
#                         help='JSON file for configuration')
#     parser.add_argument('-p', '--phase', type=str, choices=['train', 'test'],
#                         help='Run either train(training + validation) or testing', default='train')
#     parser.add_argument('-gpu', '--gpu_ids', type=str, default=None)
#     parser.add_argument('-debug', '-d', action='store_true')
#     parser.add_argument('-enable_wandb', action='store_true')
#     parser.add_argument('-log_eval', action='store_true')

#     # è§£æé…ç½®
#     args = parser.parse_args()
#     opt = Logger.parse(args)
#     opt = Logger.dict_to_nonedict(opt)

#     # è®¾ç½®æ—¥å¿—
#     torch.backends.cudnn.enabled = True
#     torch.backends.cudnn.benchmark = True

#     Logger.setup_logger(None, opt['path']['log'], 'train', level=logging.INFO, screen=True)
#     Logger.setup_logger('test', opt['path']['log'], 'test', level=logging.INFO)
#     logger = logging.getLogger('base')
#     logger.info(Logger.dict2str(opt))
#     tb_logger = SummaryWriter(log_dir=opt['path']['tb_logger'])

#     # åˆå§‹åŒ–WandbLogger
#     if opt['enable_wandb']:
#         import wandb
#         print("Initializing wandblog.")
#         wandb_logger = WandbLogger(opt)
#         wandb.define_metric('epoch')
#         wandb.define_metric('training/train_step')
#         wandb.define_metric("training/*", step_metric="train_step")
#         wandb.define_metric('validation/val_step')
#         wandb.define_metric("validation/*", step_metric="val_step")
#         train_step = 0
#         val_step = 0
#     else:
#         wandb_logger = None

#     # åŠ è½½æ•°æ®é›†
#     print("ğŸ”„ åŠ è½½æ•°æ®é›†...")
#     for phase, dataset_opt in opt['datasets'].items():
#         if phase == 'train' and args.phase != 'test':
#             print("Creating [train] change-detection dataloader.")
#             train_set = Data.create_cd_dataset(dataset_opt, phase)
#             train_loader = Data.create_cd_dataloader(train_set, dataset_opt, phase)
#             opt['len_train_dataloader'] = len(train_loader)

#         elif phase == 'val' and args.phase != 'test':
#             print("Creating [val] change-detection dataloader.")
#             val_set = Data.create_cd_dataset(dataset_opt, phase)
#             val_loader = Data.create_cd_dataloader(val_set, dataset_opt, phase)
#             opt['len_val_dataloader'] = len(val_loader)
        
#         elif phase == 'test' and args.phase == 'test':
#             print("Creating [test] change-detection dataloader.")
#             test_set = Data.create_cd_dataset(dataset_opt, phase)
#             test_loader = Data.create_cd_dataloader(test_set, dataset_opt, phase)
#             opt['len_test_dataloader'] = len(test_loader)
    
#     logger.info('Initial Dataset Finished')

#     # åŠ è½½æ¨¡å‹
#     print("ğŸ”„ åŠ è½½æ‰©æ•£æ¨¡å‹...")
#     diffusion = Model.create_model(opt)
#     logger.info('Initial Diffusion Model Finished')

#     # å¤„ç†DataParallel
#     if isinstance(diffusion.netG, nn.DataParallel):
#         diffusion.netG = diffusion.netG.module
#         print("å·²è§£åŒ…diffusionæ¨¡å‹çš„DataParallel")

#     # å¤šGPUè®¾ç½®
#     if torch.cuda.device_count() > 1:
#         print(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
#         diffusion.netG = diffusion.netG.cuda()
#         diffusion.netG = nn.DataParallel(diffusion.netG, device_ids=[0, 1, 2, 3])
        
#         # é€‚åº¦å¢åŠ batch size
#         for phase in opt['datasets']:
#             if 'batch_size' in opt['datasets'][phase]:
#                 original_bs = opt['datasets'][phase]['batch_size']
#                 # å¯ä»¥æ ¹æ®GPUæ•°é‡è°ƒæ•´
#                 # opt['datasets'][phase]['batch_size'] = original_bs * 2
#                 print(f"{phase} batch_size: {original_bs}")

#     # è®¾ç½®å™ªå£°è°ƒåº¦
#     diffusion.set_new_noise_schedule(
#         opt['model']['beta_schedule'][opt['phase']], schedule_phase=opt['phase'])
    
#     # åˆ›å»ºå˜åŒ–æ£€æµ‹æ¨¡å‹
#     print("ğŸ”„ åŠ è½½å˜åŒ–æ£€æµ‹æ¨¡å‹...")
#     change_detection = Model.create_CD_model(opt)
    
#     # å¤„ç†CDæ¨¡å‹çš„DataParallel
#     if hasattr(change_detection, 'netCD') and change_detection.netCD is not None:
#         if isinstance(change_detection.netCD, nn.DataParallel):
#             change_detection.netCD = change_detection.netCD.module
#             print("å·²è§£åŒ…CDæ¨¡å‹çš„DataParallel")
        
#         if torch.cuda.device_count() > 1:
#             change_detection.netCD = change_detection.netCD.cuda()

#     # è®¾ç½®è®­ç»ƒä¼˜åŒ–
#     use_amp = setup_training_optimization(diffusion, change_detection)
    
#     # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
#     performance_monitor = PerformanceMonitor()

#     print("ğŸš€ æ‰€æœ‰è®¾ç½®å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...\n")

#     #################
#     # è®­ç»ƒå¾ªç¯ #
#     #################
#     n_epoch = opt['train']['n_epoch']
#     # best_mF1 = 0.0
#     # start_epoch = 0
#     start_epoch, best_mF1 = load_checkpoint_if_exists(change_detection, opt)

#     # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„æ¨¡å‹
#     checkpoint_dir = opt['path'].get('checkpoint', opt['path']['models'])
#     latest_model = os.path.join(checkpoint_dir, 'latest_net_CD.pth')

#     if os.path.exists(latest_model):
#         print(f"ğŸ”„ å‘ç°ä¿å­˜çš„æ¨¡å‹: {latest_model}")
#         change_detection.load_network(latest_model)
        
#         # å°è¯•ä»æ–‡ä»¶åè§£æepochï¼ˆå¦‚æœå‘½åè§„èŒƒï¼‰
#         try:
#             # å‡è®¾æ–‡ä»¶ååŒ…å«epochä¿¡æ¯
#             import re
#             match = re.search(r'epoch_(\d+)', latest_model)
#             if match:
#                 start_epoch = int(match.group(1)) + 1
#                 print(f"âœ… ä»epoch {start_epoch}ç»§ç»­è®­ç»ƒ")
#         except:
#             print("âœ… ä»ä¿å­˜çš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆepoché‡ç½®ä¸º0ï¼‰")
#     else:
#         print("ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ")

#     if opt['phase'] == 'train':
#         # éªŒè¯è®¾å¤‡è®¾ç½®
#         device = next(diffusion.netG.parameters()).device
#         print(f"è®¾å¤‡æ£€æŸ¥: æ¨¡å‹åœ¨ {device}")
        
#         if device.type == 'cpu' and torch.cuda.is_available():
#             target_device = torch.device('cuda:0')
#             print(f"å¼ºåˆ¶å°†æ¨¡å‹ä» {device} ç§»åŠ¨åˆ° {target_device}")
#             diffusion.netG = diffusion.netG.to(target_device)
#             change_detection.netCD = change_detection.netCD.to(target_device)
#             device = next(diffusion.netG.parameters()).device
#             print(f"ç§»åŠ¨åéªŒè¯: æ¨¡å‹ç°åœ¨åœ¨ {device}")

#         for current_epoch in range(start_epoch, n_epoch):
#             epoch_start_time = time.time()
#             change_detection._clear_cache()
            
#             train_result_path = f'{opt["path"]["results"]}/train/{current_epoch}'
#             os.makedirs(train_result_path, exist_ok=True)
            
#             ################
#             ### è®­ç»ƒé˜¶æ®µ ###
#             ################
#             print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ Epoch {current_epoch}/{n_epoch-1}")
#             message = f'å­¦ä¹ ç‡: {change_detection.optCD.param_groups[0]["lr"]:.7f}'
#             logger.info(message)
            
#             for current_step, train_data in enumerate(train_loader):
#                 step_start_time = time.time()
                
#                 # é«˜æ•ˆæ‰¹é‡å¤„ç†
#                 process_batch_efficiently(train_data, diffusion, change_detection, opt, "train")
                
#                 # å®‰å…¨çš„è®­ç»ƒæ­¥éª¤
#                 success = execute_training_step(change_detection)
                
#                 if not success:
#                     print(f"è·³è¿‡æ­¥éª¤ {current_step}")
#                     continue
                
#                 # è®°å½•æ€§èƒ½
#                 step_time = time.time() - step_start_time
#                 memory_mb = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0
#                 performance_monitor.log_step(step_time, memory_mb)
                
#                 # ä¼˜åŒ–çš„æ—¥å¿—è¾“å‡º
#                 optimized_logging(current_step, current_epoch, n_epoch, train_loader, 
#                                 change_detection, logger, opt, "train", performance_monitor)
                
#                 # ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆå‡å°‘é¢‘ç‡ï¼‰
#                 save_freq = max(opt['train']['train_print_freq'] * 2, len(train_loader) // 5)
#                 if current_step % save_freq == 0:
#                     try:
#                         visuals = change_detection.get_current_visuals()
                        
#                         # ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
#                         device = train_data['A'].device
#                         visuals['pred_cm'] = visuals['pred_cm'] * 2.0 - 1.0
#                         visuals['gt_cm'] = visuals['gt_cm'] * 2.0 - 1.0
                        
#                         pred_cm_expanded = visuals['pred_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
#                         gt_cm_expanded = visuals['gt_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
                        
#                         grid_img = torch.cat((train_data['A'], train_data['B'], 
#                                             pred_cm_expanded, gt_cm_expanded), dim=0)
#                         grid_img = Metrics.tensor2img(grid_img)
                        
#                         Metrics.save_img(grid_img, 
#                             f'{train_result_path}/img_A_B_pred_gt_e{current_epoch}_b{current_step}.png')
#                     except Exception as e:
#                         print(f"ä¿å­˜å¯è§†åŒ–å¤±è´¥: {e}")
                
#                 # å®šæœŸå†…å­˜æ¸…ç†
#                 if current_step % 50 == 0:
#                     torch.cuda.empty_cache()
            
#             ### è®­ç»ƒepochæ€»ç»“ ###
#             try:
#                 change_detection._collect_epoch_states()
#                 logs = change_detection.get_current_log()
                
#                 epoch_time = time.time() - epoch_start_time
#                 message = f'[è®­ç»ƒ Epoch {current_epoch}] mF1={logs["epoch_acc"]:.5f}, ' \
#                          f'ç”¨æ—¶={epoch_time/60:.1f}åˆ†é’Ÿ'
                
#                 print(f"\nâœ… {message}")
#                 logger.info(message)
                
#                 # è¯¦ç»†æŒ‡æ ‡
#                 for k, v in logs.items():
#                     tb_logger.add_scalar(f'train/{k}', v, current_epoch)
                
#                 if wandb_logger:
#                     wandb_logger.log_metrics({
#                         'training/mF1': logs['epoch_acc'],
#                         'training/mIoU': logs['miou'],
#                         'training/OA': logs['acc'],
#                         'training/change-F1': logs['F1_1'],
#                         'training/no-change-F1': logs['F1_0'],
#                         'training/change-IoU': logs['iou_1'],
#                         'training/no-change-IoU': logs['iou_0'],
#                         'training/train_step': current_epoch,
#                         'training/loss': logs.get('l_cd'),
#                     })
                    
#             except Exception as e:
#                 print(f"è®­ç»ƒæŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
            
#             change_detection._clear_cache()
#             change_detection._update_lr_schedulers()
            
#             ##################
#             ### éªŒè¯é˜¶æ®µ ###
#             ##################
#             if current_epoch % opt['train']['val_freq'] == 0:
#                 print(f"\nğŸ” å¼€å§‹éªŒè¯ Epoch {current_epoch}")
#                 val_start_time = time.time()
                
#                 val_result_path = f'{opt["path"]["results"]}/val/{current_epoch}'
#                 os.makedirs(val_result_path, exist_ok=True)

#                 for current_step, val_data in enumerate(val_loader):
#                     with torch.no_grad():  # éªŒè¯æ—¶ä¸éœ€è¦æ¢¯åº¦
#                         process_batch_efficiently(val_data, diffusion, change_detection, opt, "val")
#                         change_detection.test()
#                         change_detection._collect_running_batch_states()
                    
#                     # éªŒè¯æ—¥å¿—ï¼ˆå‡å°‘é¢‘ç‡ï¼‰
#                     optimized_logging(current_step, current_epoch, n_epoch, val_loader, 
#                                     change_detection, logger, opt, "val")
                    
#                     # éªŒè¯å¯è§†åŒ–ï¼ˆæ›´å°‘é¢‘ç‡ï¼‰
#                     if current_step % max(1, len(val_loader) // 3) == 0:
#                         try:
#                             visuals = change_detection.get_current_visuals()
#                             device = val_data['A'].device
#                             visuals['pred_cm'] = visuals['pred_cm'] * 2.0 - 1.0
#                             visuals['gt_cm'] = visuals['gt_cm'] * 2.0 - 1.0
                            
#                             pred_cm_expanded = visuals['pred_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
#                             gt_cm_expanded = visuals['gt_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
                            
#                             grid_img = torch.cat((val_data['A'], val_data['B'], 
#                                                 pred_cm_expanded, gt_cm_expanded), dim=0)
#                             grid_img = Metrics.tensor2img(grid_img)
                            
#                             Metrics.save_img(grid_img, 
#                                 f'{val_result_path}/img_A_B_pred_gt_e{current_epoch}_b{current_step}.png')
#                         except Exception as e:
#                             print(f"éªŒè¯å¯è§†åŒ–å¤±è´¥: {e}")

#                 ### éªŒè¯æ€»ç»“ ### 
#                 try:
#                     change_detection._collect_epoch_states()
#                     logs = change_detection.get_current_log()
                    
#                     val_time = time.time() - val_start_time
#                     message = f'[éªŒè¯ Epoch {current_epoch}] mF1={logs["epoch_acc"]:.5f}, ' \
#                             f'ç”¨æ—¶={val_time/60:.1f}åˆ†é’Ÿ'
                    
#                     print(f"âœ… {message}")
#                     logger.info(message)
                    
#                     for k, v in logs.items():
#                         tb_logger.add_scalar(f'val/{k}', v, current_epoch)

#                     # ğŸ” è¯¦ç»†çš„WandBè°ƒè¯•è®°å½•
#                     if wandb_logger:
#                         try:
#                             # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰logs
#                             print("\nğŸ” === WandBè°ƒè¯•ä¿¡æ¯ ===")
#                             print(f"å½“å‰epoch: {current_epoch}")
#                             print(f"best_mF1: {best_mF1} (ç±»å‹: {type(best_mF1)})")
#                             print("logså†…å®¹:")
#                             for k, v in logs.items():
#                                 print(f"  {k}: {v} (ç±»å‹: {type(v)})")
                            
#                             # å®‰å…¨è½¬æ¢æ‰€æœ‰æŒ‡æ ‡
#                             def safe_convert(value, key):
#                                 if value is None:
#                                     print(f"  âš ï¸  {key}: Noneå€¼")
#                                     return None
#                                 try:
#                                     if hasattr(value, 'item'):  # PyTorch tensor
#                                         result = float(value.item())
#                                     else:
#                                         result = float(value)
                                    
#                                     # æ£€æŸ¥NaNå’Œæ— ç©·å¤§
#                                     if result != result or result == float('inf') or result == float('-inf'):
#                                         print(f"  âŒ {key}: æ— æ•ˆæ•°å€¼ {result}")
#                                         return None
                                    
#                                     print(f"  âœ… {key}: {value} â†’ {result}")
#                                     return result
#                                 except Exception as e:
#                                     print(f"  âŒ {key}: è½¬æ¢å¤±è´¥ {value} - {e}")
#                                     return None
                            
#                             # æ„å»ºå®‰å…¨çš„æŒ‡æ ‡å­—å…¸
#                             validation_metrics = {}
                            
#                             # ä¸»è¦æŒ‡æ ‡
#                             for wandb_key, log_key in [
#                                 ('validation/mF1', 'epoch_acc'),
#                                 ('validation/loss', 'l_cd'),
#                                 ('validation/mIoU', 'miou'),
#                                 ('validation/accuracy', 'acc'),
#                                 ('validation/change_F1', 'F1_1'),
#                                 ('validation/no_change_F1', 'F1_0'),
#                                 ('validation/change_IoU', 'iou_1'),
#                                 ('validation/no_change_IoU', 'iou_0'),
#                             ]:
#                                 converted = safe_convert(logs.get(log_key), wandb_key)
#                                 if converted is not None:
#                                     validation_metrics[wandb_key] = converted
                            
#                             # ç®€åŒ–å‘½åçš„æŒ‡æ ‡
#                             for wandb_key, log_key in [
#                                 ('val_mF1', 'epoch_acc'),
#                                 ('val_loss', 'l_cd'),
#                                 ('val_mIoU', 'miou'),
#                                 ('val_accuracy', 'acc'),
#                             ]:
#                                 converted = safe_convert(logs.get(log_key), wandb_key)
#                                 if converted is not None:
#                                     validation_metrics[wandb_key] = converted
                            
#                             # å…¶ä»–æŒ‡æ ‡
#                             validation_metrics['epoch'] = current_epoch
#                             validation_metrics['validation_step'] = current_epoch
                            
#                             # best_mF1
#                             converted_best = safe_convert(best_mF1, 'val_best_mF1')
#                             if converted_best is not None:
#                                 validation_metrics['val_best_mF1'] = converted_best
#                                 validation_metrics['validation/best_mF1'] = converted_best
                            
#                             # æ—¶é—´
#                             validation_metrics['validation/time_minutes'] = val_time / 60
                            
#                             print(f"\nğŸ“Š å°†è¦è®°å½•çš„æŒ‡æ ‡ ({len(validation_metrics)}ä¸ª):")
#                             for k, v in validation_metrics.items():
#                                 print(f"  {k}: {v}")
                            
#                             # è®°å½•åˆ°WandB
#                             if validation_metrics:
#                                 wandb_logger.log_metrics(validation_metrics)
#                                 print(f"\nâœ… WandBè®°å½•æˆåŠŸ: {len(validation_metrics)}ä¸ªæŒ‡æ ‡")
#                             else:
#                                 print("\nâŒ æ²¡æœ‰æœ‰æ•ˆæŒ‡æ ‡å¯è®°å½•")
                            
#                             print("ğŸ” === WandBè°ƒè¯•ä¿¡æ¯ç»“æŸ ===\n")
                            
#                         except Exception as e:
#                             print(f"âŒ WandBè®°å½•é”™è¯¯: {e}")
#                             import traceback
#                             traceback.print_exc()
                    
#                     # æ¨¡å‹ä¿å­˜é€»è¾‘ä¿æŒä¸å˜
#                     if logs['epoch_acc'] > best_mF1:
#                         is_best_model = True
#                         best_mF1 = logs['epoch_acc']
#                         print(f"ğŸ‰ æœ€ä½³æ¨¡å‹æ›´æ–°! mF1: {best_mF1:.5f}")
#                         logger.info('[éªŒè¯] æœ€ä½³æ¨¡å‹æ›´æ–°ï¼Œä¿å­˜æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€')
#                     else:
#                         is_best_model = False
#                         logger.info('[éªŒè¯] ä¿å­˜å½“å‰æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€')

#                     change_detection.save_network(current_epoch, is_best_model=is_best_model)
                    
#                 except Exception as e:
#                     print(f"éªŒè¯æŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
                
#                 change_detection._clear_cache()
#                 print(f"--- è¿›å…¥ä¸‹ä¸€ä¸ªEpoch ---\n")

#             if wandb_logger:
#                 wandb_logger.log_metrics({'epoch': current_epoch})
            
#             # Epochç»“æŸæ¸…ç†
#             torch.cuda.empty_cache()
                
#         print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
#         logger.info('è®­ç»ƒç»“æŸ')
        
#     else:
#         ##################
#         ### æµ‹è¯•é˜¶æ®µ ###
#         ##################
#         logger.info('å¼€å§‹æ¨¡å‹è¯„ä¼°ï¼ˆæµ‹è¯•ï¼‰')
#         print("ğŸ” å¼€å§‹æµ‹è¯•...")
        
#         test_result_path = f'{opt["path"]["results"]}/test/'
#         os.makedirs(test_result_path, exist_ok=True)
#         logger_test = logging.getLogger('test')
#         change_detection._clear_cache()
        
#         for current_step, test_data in enumerate(test_loader):
#             with torch.no_grad():
#                 process_batch_efficiently(test_data, diffusion, change_detection, opt, "test")
#                 change_detection.test()
#                 change_detection._collect_running_batch_states()

#             # æµ‹è¯•æ—¥å¿—
#             if current_step % max(1, len(test_loader) // 10) == 0:
#                 logs = change_detection.get_current_log()
#                 message = f'[æµ‹è¯•] Step {current_step}/{len(test_loader)}, ' \
#                          f'mF1: {logs["running_acc"]:.5f}'
#                 print(message)
#                 logger_test.info(message)

#             # ä¿å­˜æµ‹è¯•ç»“æœ
#             try:
#                 visuals = change_detection.get_current_visuals()
#                 visuals['pred_cm'] = visuals['pred_cm'] * 2.0 - 1.0
#                 visuals['gt_cm'] = visuals['gt_cm'] * 2.0 - 1.0
                
#                 # å•ç‹¬ä¿å­˜å›¾åƒ
#                 img_A = Metrics.tensor2img(test_data['A'], out_type=np.uint8, min_max=(-1, 1))
#                 img_B = Metrics.tensor2img(test_data['B'], out_type=np.uint8, min_max=(-1, 1))
#                 gt_cm = Metrics.tensor2img(visuals['gt_cm'].unsqueeze(1).repeat(1, 3, 1, 1), 
#                                           out_type=np.uint8, min_max=(0, 1))
#                 pred_cm = Metrics.tensor2img(visuals['pred_cm'].unsqueeze(1).repeat(1, 3, 1, 1), 
#                                             out_type=np.uint8, min_max=(0, 1))

#                 Metrics.save_img(img_A, f'{test_result_path}/img_A_{current_step}.png')
#                 Metrics.save_img(img_B, f'{test_result_path}/img_B_{current_step}.png')
#                 Metrics.save_img(pred_cm, f'{test_result_path}/img_pred_cm{current_step}.png')
#                 Metrics.save_img(gt_cm, f'{test_result_path}/img_gt_cm{current_step}.png')
                
#             except Exception as e:
#                 print(f"æµ‹è¯•ä¿å­˜å¤±è´¥: {e}")

#         ### æµ‹è¯•æ€»ç»“ ###
#         try:
#             change_detection._collect_epoch_states()
#             logs = change_detection.get_current_log()
            
#             message = f'[æµ‹è¯•æ€»ç»“] mF1={logs["epoch_acc"]:.5f}\n'
#             for k, v in logs.items():
#                 message += f'{k}: {v:.4e} '
#             message += '\n'
            
#             print(f"âœ… {message}")
#             logger_test.info(message)

#             if wandb_logger:
#                 wandb_logger.log_metrics({
#                     'test/mF1': logs['epoch_acc'],
#                     'test/mIoU': logs['miou'],
#                     'test/OA': logs['acc'],
#                     'test/change-F1': logs['F1_1'],
#                     'test/no-change-F1': logs['F1_0'],
#                     'test/change-IoU': logs['iou_1'],
#                     'test/no-change-IoU': logs['iou_0'],
#                 })
                
#         except Exception as e:
#             print(f"æµ‹è¯•æŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")

#         print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
#         logger.info('æµ‹è¯•ç»“æŸ')

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒ
# import torch.multiprocessing as mp  # å¤šè¿›ç¨‹
# from torch.nn.parallel import DistributedDataParallel as DDP  # åˆ†å¸ƒå¼DataParallel
# from torch.utils.data.distributed import DistributedSampler  # åˆ†å¸ƒå¼é‡‡æ ·å™¨

# import data as Data
# import model as Model
# import argparse
# import logging
# import core.logger as Logger
# import core.metrics as Metrics
# from core.wandb_logger import WandbLogger
# from tensorboardX import SummaryWriter
# import os
# import numpy as np
# from model.cd_modules.cd_head import cd_head 
# from misc.print_diffuse_feats import print_feats
# import time
# import datetime
# from contextlib import contextmanager

# # ==================== DDPå…¼å®¹æ€§å·¥å…· ====================
# def safe_model_call(model, method_name, *args, **kwargs):
#     """å®‰å…¨è°ƒç”¨è¢«DDP/DPåŒ…è£…çš„æ¨¡å‹æ–¹æ³•"""
#     # è·å–å®é™…çš„æ¨¡å‹
#     actual_model = model
#     if isinstance(model, (DDP, nn.DataParallel)):
#         actual_model = model.module
    
#     # æ£€æŸ¥æ–¹æ³•æ˜¯å¦å­˜åœ¨
#     if hasattr(actual_model, method_name):
#         method = getattr(actual_model, method_name)
#         return method(*args, **kwargs)
#     else:
#         raise AttributeError(f"æ¨¡å‹æ²¡æœ‰æ–¹æ³• '{method_name}'")

# def get_actual_model(model):
#     """è·å–è¢«DDP/DPåŒ…è£…çš„å®é™…æ¨¡å‹"""
#     if isinstance(model, (DDP, nn.DataParallel)):
#         return model.module
#     return model

# # ==================== å¤šGPUé…ç½®ç±» ====================
# class MultiGPUConfig:
#     """å¤šGPUè®­ç»ƒé…ç½®ç®¡ç†"""
#     def __init__(self, args, opt):
#         self.args = args
#         self.opt = opt
#         self.world_size = torch.cuda.device_count()
#         self.use_ddp = args.use_ddp if hasattr(args, 'use_ddp') else False
#         self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
#         self.rank = int(os.environ.get('RANK', 0))
        
#         # æ ¹æ®GPUæ•°é‡é€‰æ‹©ç­–ç•¥
#         if self.world_size <= 1:
#             self.strategy = 'single'
#         elif self.use_ddp:
#             self.strategy = 'ddp'
#         else:
#             self.strategy = 'dp'
        
#         print(f"ğŸ”§ å¤šGPUé…ç½®: {self.strategy}, GPUæ•°é‡: {self.world_size}")

# # ==================== æ”¹è¿›çš„åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ– ====================
# def setup_distributed_training(rank, world_size, port='29500'):
#     """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ - æ”¹è¿›ç‰ˆæœ¬"""
#     print(f"ğŸ”„ [GPU {rank}] å¼€å§‹åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ...")
    
#     try:
#         # è®¾ç½®ç¯å¢ƒå˜é‡
#         os.environ['MASTER_ADDR'] = '127.0.0.1'  # ä½¿ç”¨127.0.0.1è€Œä¸æ˜¯localhost
#         os.environ['MASTER_PORT'] = str(port)
#         os.environ['WORLD_SIZE'] = str(world_size)
#         os.environ['RANK'] = str(rank)
        
#         print(f"   ğŸ“‹ [GPU {rank}] ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")
#         print(f"   ğŸ“‹ [GPU {rank}] MASTER_ADDR: {os.environ['MASTER_ADDR']}")
#         print(f"   ğŸ“‹ [GPU {rank}] MASTER_PORT: {os.environ['MASTER_PORT']}")
#         print(f"   ğŸ“‹ [GPU {rank}] WORLD_SIZE: {world_size}, RANK: {rank}")
        
#         # è®¾ç½®CUDAè®¾å¤‡
#         print(f"   ğŸ”„ [GPU {rank}] è®¾ç½®CUDAè®¾å¤‡...")
#         torch.cuda.set_device(rank)
#         print(f"   âœ… [GPU {rank}] CUDAè®¾å¤‡è®¾ç½®ä¸º: {torch.cuda.current_device()}")
        
#         # åˆå§‹åŒ–è¿›ç¨‹ç»„ - æ·»åŠ æ›´å¤šé”™è¯¯å¤„ç†
#         print(f"   ğŸ”„ [GPU {rank}] åˆå§‹åŒ–è¿›ç¨‹ç»„...")
        
#         # å°è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•
#         init_methods = [
#             f'tcp://127.0.0.1:{port}',
#             f'tcp://localhost:{port}',
#             'env://'
#         ]
        
#         success = False
#         for init_method in init_methods:
#             try:
#                 print(f"   ğŸ”„ [GPU {rank}] å°è¯•åˆå§‹åŒ–æ–¹æ³•: {init_method}")
                
#                 dist.init_process_group(
#                     backend='nccl',
#                     init_method=init_method,
#                     world_size=world_size,
#                     rank=rank,
#                     timeout=datetime.timedelta(seconds=30)  # è®¾ç½®è¶…æ—¶
#                 )
                
#                 print(f"   âœ… [GPU {rank}] è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ–¹æ³•: {init_method}")
#                 success = True
#                 break
                
#             except Exception as e:
#                 print(f"   âŒ [GPU {rank}] åˆå§‹åŒ–æ–¹æ³• {init_method} å¤±è´¥: {e}")
#                 continue
        
#         if not success:
#             print(f"   âŒ [GPU {rank}] æ‰€æœ‰åˆå§‹åŒ–æ–¹æ³•éƒ½å¤±è´¥")
#             return False
        
#         # éªŒè¯åˆ†å¸ƒå¼è®¾ç½®
#         print(f"   ğŸ”„ [GPU {rank}] éªŒè¯åˆ†å¸ƒå¼è®¾ç½®...")
#         if dist.is_initialized():
#             print(f"   âœ… [GPU {rank}] åˆ†å¸ƒå¼å·²åˆå§‹åŒ–")
#             print(f"   ğŸ“‹ [GPU {rank}] World size: {dist.get_world_size()}")
#             print(f"   ğŸ“‹ [GPU {rank}] Rank: {dist.get_rank()}")
            
#             # ç®€å•çš„é€šä¿¡æµ‹è¯•
#             print(f"   ğŸ”„ [GPU {rank}] è¿›è¡Œé€šä¿¡æµ‹è¯•...")
#             test_tensor = torch.tensor([rank], device=f'cuda:{rank}')
#             dist.all_reduce(test_tensor, op=dist.ReduceOp.SUM)
#             expected_sum = sum(range(world_size))
            
#             if test_tensor.item() == expected_sum:
#                 print(f"   âœ… [GPU {rank}] é€šä¿¡æµ‹è¯•æˆåŠŸ: {test_tensor.item()} == {expected_sum}")
#             else:
#                 print(f"   âŒ [GPU {rank}] é€šä¿¡æµ‹è¯•å¤±è´¥: {test_tensor.item()} != {expected_sum}")
#                 return False
#         else:
#             print(f"   âŒ [GPU {rank}] åˆ†å¸ƒå¼æœªæ­£ç¡®åˆå§‹åŒ–")
#             return False
        
#         print(f"âœ… [GPU {rank}] åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–å®Œæˆ")
#         return True
        
#     except Exception as e:
#         print(f"âŒ [GPU {rank}] åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–å¤±è´¥: {e}")
#         import traceback
#         traceback.print_exc()
#         return False
    
# # ==================== è°ƒè¯•ç”¨çš„åˆ†å¸ƒå¼çŠ¶æ€æ£€æŸ¥ ====================
# def debug_distributed_state(rank):
#     """è°ƒè¯•åˆ†å¸ƒå¼çŠ¶æ€"""
#     print(f"\nğŸ” [GPU {rank}] === åˆ†å¸ƒå¼çŠ¶æ€è°ƒè¯• ===")
    
#     # æ£€æŸ¥ç¯å¢ƒå˜é‡
#     env_vars = ['MASTER_ADDR', 'MASTER_PORT', 'WORLD_SIZE', 'RANK', 'LOCAL_RANK']
#     for var in env_vars:
#         value = os.environ.get(var, 'Not set')
#         print(f"   {var}: {value}")
    
#     # æ£€æŸ¥CUDAçŠ¶æ€
#     print(f"   CUDA available: {torch.cuda.is_available()}")
#     print(f"   CUDA device count: {torch.cuda.device_count()}")
#     print(f"   Current CUDA device: {torch.cuda.current_device()}")
    
#     # æ£€æŸ¥åˆ†å¸ƒå¼çŠ¶æ€
#     print(f"   Distributed initialized: {dist.is_initialized()}")
#     if dist.is_initialized():
#         print(f"   Backend: {dist.get_backend()}")
#         print(f"   World size: {dist.get_world_size()}")
#         print(f"   Rank: {dist.get_rank()}")
    
#     print(f"ğŸ” [GPU {rank}] === è°ƒè¯•ä¿¡æ¯ç»“æŸ ===\n")

# # ==================== å¸¦è¶…æ—¶çš„å®‰å…¨DDPåŒ…è£… ====================
# def safe_ddp_wrap_with_timeout(model, device_ids, output_device, timeout_seconds=60):
#     """å¸¦è¶…æ—¶çš„å®‰å…¨DDPåŒ…è£…"""
#     import signal
#     import threading
    
#     result = [None]
#     exception = [None]
    
#     def wrap_model():
#         try:
#             wrapped_model = DDP(
#                 model,
#                 device_ids=device_ids,
#                 output_device=output_device,
#                 find_unused_parameters=False
#             )
#             result[0] = wrapped_model
#         except Exception as e:
#             exception[0] = e
    
#     # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡ŒåŒ…è£…
#     thread = threading.Thread(target=wrap_model)
#     thread.start()
#     thread.join(timeout=timeout_seconds)
    
#     if thread.is_alive():
#         print(f"   âš ï¸  DDPåŒ…è£…è¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰ï¼Œå¯èƒ½å­˜åœ¨æ­»é”")
#         return None
    
#     if exception[0]:
#         print(f"   âŒ DDPåŒ…è£…å¼‚å¸¸: {exception[0]}")
#         return None
    
#     return result[0]

# def cleanup_distributed():
#     """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
#     if dist.is_initialized():
#         dist.destroy_process_group()

# # ==================== å¤šGPUæ¨¡å‹åŒ…è£…å™¨ ====================
# class MultiGPUModelWrapper:
#     """å¤šGPUæ¨¡å‹åŒ…è£…å™¨"""
    
#     def __init__(self, config):
#         self.config = config
#         self.is_main_process = (config.rank == 0) if config.use_ddp else True
    
#     def wrap_diffusion_model(self, diffusion):
#         """åŒ…è£…æ‰©æ•£æ¨¡å‹"""
#         print(f"ğŸ”§ åŒ…è£…æ‰©æ•£æ¨¡å‹: {self.config.strategy}")
        
#         # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
#         if torch.cuda.is_available():
#             if self.config.strategy == 'ddp':
#                 device = torch.device(f'cuda:{self.config.local_rank}')
#                 diffusion.netG = diffusion.netG.to(device)
                
#                 # åŒ…è£…ä¸ºDDP
#                 diffusion.netG = DDP(
#                     diffusion.netG, 
#                     device_ids=[self.config.local_rank],
#                     output_device=self.config.local_rank,
#                     find_unused_parameters=True
#                 )
#                 print(f"âœ… [GPU {self.config.rank}] æ‰©æ•£æ¨¡å‹DDPåŒ…è£…å®Œæˆ")
                
#             elif self.config.strategy == 'dp' and self.config.world_size > 1:
#                 # è§£åŒ…ç°æœ‰çš„DataParallelï¼ˆå¦‚æœå­˜åœ¨ï¼‰
#                 if isinstance(diffusion.netG, nn.DataParallel):
#                     diffusion.netG = diffusion.netG.module
                
#                 device_ids = list(range(self.config.world_size))
#                 diffusion.netG = diffusion.netG.cuda()
#                 diffusion.netG = nn.DataParallel(diffusion.netG, device_ids=device_ids)
#                 print(f"âœ… æ‰©æ•£æ¨¡å‹DataParallelåŒ…è£…å®Œæˆï¼Œä½¿ç”¨GPU: {device_ids}")
                
#             else:
#                 # å•GPU
#                 diffusion.netG = diffusion.netG.cuda()
#                 print("âœ… æ‰©æ•£æ¨¡å‹å•GPUè®¾ç½®å®Œæˆ")
        
#         return diffusion
    
#     def wrap_change_detection_model(self, change_detection):
#         """åŒ…è£…å˜åŒ–æ£€æµ‹æ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬"""
#         print(f"ğŸ”§ åŒ…è£…å˜åŒ–æ£€æµ‹æ¨¡å‹: {self.config.strategy}")
        
#         if not hasattr(change_detection, 'netCD') or change_detection.netCD is None:
#             print("âš ï¸  å˜åŒ–æ£€æµ‹æ¨¡å‹ä¸å­˜åœ¨netCDå±æ€§")
#             return change_detection
        
#         if torch.cuda.is_available():
#             if self.config.strategy == 'ddp':
#                 device = torch.device(f'cuda:{self.config.local_rank}')
#                 print(f"   ğŸ“ [GPU {self.config.rank}] ç›®æ ‡è®¾å¤‡: {device}")
                
#                 # 1. é¦–å…ˆç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
#                 print(f"   ğŸ”„ [GPU {self.config.rank}] ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡...")
#                 try:
#                     change_detection.netCD = change_detection.netCD.to(device)
#                     print(f"   âœ… [GPU {self.config.rank}] æ¨¡å‹ç§»åŠ¨å®Œæˆ")
#                 except Exception as e:
#                     print(f"   âŒ [GPU {self.config.rank}] æ¨¡å‹ç§»åŠ¨å¤±è´¥: {e}")
#                     return change_detection
                
#                 # 2. å¤„ç†ä¼˜åŒ–å™¨ï¼ˆå¦‚æœå­˜åœ¨ä¸”å·²æœ‰çŠ¶æ€ï¼‰
#                 print(f"   ğŸ”„ [GPU {self.config.rank}] æ£€æŸ¥ä¼˜åŒ–å™¨...")
#                 if hasattr(change_detection, 'optCD') and change_detection.optCD is not None:
#                     # åªæœ‰å½“ä¼˜åŒ–å™¨æœ‰çŠ¶æ€æ—¶æ‰ç§»åŠ¨
#                     if len(change_detection.optCD.state) > 0:
#                         print(f"   ğŸ”„ [GPU {self.config.rank}] ç§»åŠ¨ä¼˜åŒ–å™¨çŠ¶æ€...")
#                         try:
#                             for state in change_detection.optCD.state.values():
#                                 for k, v in state.items():
#                                     if torch.is_tensor(v):
#                                         state[k] = v.to(device)
#                             print(f"   âœ… [GPU {self.config.rank}] ä¼˜åŒ–å™¨çŠ¶æ€ç§»åŠ¨å®Œæˆ")
#                         except Exception as e:
#                             print(f"   âš ï¸  [GPU {self.config.rank}] ä¼˜åŒ–å™¨çŠ¶æ€ç§»åŠ¨å¤±è´¥: {e}")
#                     else:
#                         print(f"   â„¹ï¸  [GPU {self.config.rank}] ä¼˜åŒ–å™¨æ— çŠ¶æ€ï¼Œè·³è¿‡ç§»åŠ¨")
                
#                 # 3. æ·»åŠ åŒæ­¥ç‚¹ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆäº†æ¨¡å‹ç§»åŠ¨
#                 print(f"   ğŸ”„ [GPU {self.config.rank}] ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆæ¨¡å‹ç§»åŠ¨...")
#                 try:
#                     if dist.is_initialized():
#                         dist.barrier()
#                         print(f"   âœ… [GPU {self.config.rank}] åŒæ­¥å®Œæˆ")
#                 except Exception as e:
#                     print(f"   âš ï¸  [GPU {self.config.rank}] åŒæ­¥å¤±è´¥: {e}")
                
#                 # 4. åŒ…è£…ä¸ºDDP - ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
#                 print(f"   ğŸ”„ [GPU {self.config.rank}] åˆ›å»ºDDPåŒ…è£…...")
#                 try:
#                     change_detection.netCD = DDP(
#                         change_detection.netCD,
#                         device_ids=[self.config.local_rank],
#                         output_device=self.config.local_rank,
#                         find_unused_parameters=False,  # æ”¹ä¸ºFalseï¼Œé¿å…é¢å¤–æ£€æŸ¥
#                         broadcast_buffers=False,
#                         gradient_as_bucket_view=False,  # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
#                         static_graph = True
#                     )
#                     print(f"   âœ… [GPU {self.config.rank}] DDPåŒ…è£…å®Œæˆ")
#                 except Exception as e:
#                     print(f"   âŒ [GPU {self.config.rank}] DDPåŒ…è£…å¤±è´¥: {e}")
#                     print(f"   ğŸ”„ [GPU {self.config.rank}] å°è¯•åŸºæœ¬DDPé…ç½®...")
#                     try:
#                         change_detection.netCD = DDP(
#                             change_detection.netCD,
#                             device_ids=[self.config.local_rank],
#                             output_device=self.config.local_rank
#                         )
#                         print(f"   âœ… [GPU {self.config.rank}] åŸºæœ¬DDPåŒ…è£…å®Œæˆ")
#                     except Exception as e2:
#                         print(f"   âŒ [GPU {self.config.rank}] åŸºæœ¬DDPä¹Ÿå¤±è´¥: {e2}")
#                         print(f"   ğŸ”„ [GPU {self.config.rank}] å›é€€åˆ°å•GPUæ¨¡å¼...")
#                         # å›é€€åˆ°å•GPU
#                         return change_detection
                
#                 print(f"âœ… [GPU {self.config.rank}] å˜åŒ–æ£€æµ‹æ¨¡å‹DDPåŒ…è£…å®Œæˆ")
                
#             elif self.config.strategy == 'dp' and self.config.world_size > 1:
#                 # DataParallelåŒ…è£…
#                 if isinstance(change_detection.netCD, nn.DataParallel):
#                     change_detection.netCD = change_detection.netCD.module
                
#                 device_ids = list(range(self.config.world_size))
#                 change_detection.netCD = change_detection.netCD.cuda()
#                 change_detection.netCD = nn.DataParallel(change_detection.netCD, device_ids=device_ids)
#                 print(f"âœ… å˜åŒ–æ£€æµ‹æ¨¡å‹DataParallelåŒ…è£…å®Œæˆï¼Œä½¿ç”¨GPU: {device_ids}")
                
#             else:
#                 # å•GPU
#                 change_detection.netCD = change_detection.netCD.cuda()
#                 print("âœ… å˜åŒ–æ£€æµ‹æ¨¡å‹å•GPUè®¾ç½®å®Œæˆ")
        
#         return change_detection

# # ==================== å¤šGPUæ•°æ®åŠ è½½å™¨ ====================
# def create_multi_gpu_dataloader(dataset, dataset_opt, phase, config):
#     """åˆ›å»ºå¤šGPUæ•°æ®åŠ è½½å™¨"""
    
#     if config.strategy == 'ddp':
#         # DDPéœ€è¦åˆ†å¸ƒå¼é‡‡æ ·å™¨
#         sampler = DistributedSampler(
#             dataset,
#             num_replicas=config.world_size,
#             rank=config.rank,
#             shuffle=(phase == 'train')
#         )
        
#         # è°ƒæ•´batch sizeï¼ˆæ¯ä¸ªGPUçš„batch sizeï¼‰
#         batch_size = dataset_opt['batch_size'] // config.world_size
#         if batch_size == 0:
#             batch_size = 1
#             print(f"âš ï¸  batch_sizeå¤ªå°ï¼Œæ¯ä¸ªGPUä½¿ç”¨batch_size=1")
        
#         dataloader = torch.utils.data.DataLoader(
#             dataset,
#             batch_size=batch_size,
#             sampler=sampler,
#             num_workers=dataset_opt.get('num_workers', 4),
#             pin_memory=True,
#             drop_last=(phase == 'train')
#         )
        
#         print(f"âœ… [GPU {config.rank}] DDPæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œbatch_size={batch_size}")
        
#     else:
#         # DataParallelæˆ–å•GPUä½¿ç”¨æ ‡å‡†æ•°æ®åŠ è½½å™¨
#         if config.strategy == 'dp' and config.world_size > 1:
#             # DataParallelå¯ä»¥å¢åŠ batch size
#             batch_size = dataset_opt['batch_size'] * config.world_size
#             print(f"âœ… DataParallelå¢åŠ batch_size: {dataset_opt['batch_size']} â†’ {batch_size}")
#         else:
#             batch_size = dataset_opt['batch_size']
        
#         dataloader = torch.utils.data.DataLoader(
#             dataset,
#             batch_size=batch_size,
#             shuffle=(phase == 'train'),
#             num_workers=dataset_opt.get('num_workers', 4),
#             pin_memory=True,
#             drop_last=(phase == 'train')
#         )
    
#     return dataloader, sampler if config.strategy == 'ddp' else None

# # ==================== å¤šGPUæ€§èƒ½ç›‘æ§ ====================
# class MultiGPUPerformanceMonitor:
#     """å¤šGPUæ€§èƒ½ç›‘æ§"""
    
#     def __init__(self, config):
#         self.config = config
#         self.step_times = []
#         self.memory_usage = []
#         self.start_time = time.time()
#         self.is_main_process = (config.rank == 0) if config.use_ddp else True
    
#     def log_step(self, step_time, memory_mb=None):
#         if self.is_main_process:
#             self.step_times.append(step_time)
#             if memory_mb:
#                 self.memory_usage.append(memory_mb)
    
#     def get_stats(self):
#         if not self.is_main_process or not self.step_times:
#             return ""
        
#         avg_time = np.mean(self.step_times[-100:])
#         total_time = time.time() - self.start_time
        
#         stats = f"å¹³å‡æ­¥æ—¶: {avg_time:.2f}s, æ€»æ—¶é—´: {total_time/60:.1f}min"
        
#         if self.memory_usage:
#             avg_memory = np.mean(self.memory_usage[-10:])
#             stats += f", GPU{self.config.local_rank}æ˜¾å­˜: {avg_memory:.1f}MB"
        
#         # æ·»åŠ å¤šGPUä¿¡æ¯
#         if self.config.world_size > 1:
#             stats += f" [ç­–ç•¥: {self.config.strategy.upper()}, {self.config.world_size}GPU]"
        
#         return stats

# # ==================== å¤šGPUå®‰å…¨è®­ç»ƒæ­¥éª¤ ====================
# def safe_multi_gpu_training_step(change_detection, config):
#     """å¤šGPUå®‰å…¨è®­ç»ƒæ­¥éª¤"""
#     try:
#         change_detection.optimize_parameters()
#         change_detection._collect_running_batch_states()
        
#         # DDPéœ€è¦åŒæ­¥
#         if config.strategy == 'ddp':
#             dist.barrier()
        
#         return True
        
#     except RuntimeError as e:
#         if "device-side assert triggered" in str(e) or "CUDA" in str(e):
#             if config.rank == 0 or not config.use_ddp:
#                 print(f"âš ï¸  [GPU {config.local_rank}] CUDAé”™è¯¯å·²è‡ªåŠ¨å¤„ç†: {str(e)[:100]}...")
#             torch.cuda.empty_cache()
#             return False
#         else:
#             raise
#     except Exception as e:
#         if config.rank == 0 or not config.use_ddp:
#             print(f"âŒ [GPU {config.local_rank}] è®­ç»ƒæ­¥éª¤é”™è¯¯: {e}")
#         return False

# # ==================== å¤šGPUæ¨¡å‹ä¿å­˜å’ŒåŠ è½½ ====================
# class MultiGPUCheckpointManager:
#     """å¤šGPU checkpointç®¡ç†å™¨"""
    
#     def __init__(self, config):
#         self.config = config
#         self.is_main_process = (config.rank == 0) if config.use_ddp else True
    
#     def save_checkpoint(self, change_detection, epoch, is_best_model=False):
#         """ä¿å­˜checkpoint"""
#         if not self.is_main_process:
#             return
        
#         try:
#             # è·å–åŸå§‹æ¨¡å‹ï¼ˆå»é™¤DDP/DPåŒ…è£…ï¼‰
#             if hasattr(change_detection, 'netCD'):
#                 if isinstance(change_detection.netCD, (DDP, nn.DataParallel)):
#                     model_state_dict = change_detection.netCD.module.state_dict()
#                 else:
#                     model_state_dict = change_detection.netCD.state_dict()
#             else:
#                 print("âš ï¸  æ— æ³•æ‰¾åˆ°netCDæ¨¡å‹")
#                 return
            
#             # ä¿å­˜è·¯å¾„
#             checkpoint_dir = os.path.expanduser("~/bowen/DDPM-CD/ddpm-cd/checkpoint/gvlm_cd_ddpm")
#             os.makedirs(checkpoint_dir, exist_ok=True)
            
#             # ä¿å­˜æ¨¡å‹
#             gen_file = os.path.join(checkpoint_dir, f"cd_model_E{epoch}_gen.pth")
#             torch.save(model_state_dict, gen_file)
            
#             # ä¿å­˜ä¼˜åŒ–å™¨
#             if hasattr(change_detection, 'optCD') and change_detection.optCD is not None:
#                 opt_file = os.path.join(checkpoint_dir, f"cd_model_E{epoch}_opt.pth")
#                 torch.save(change_detection.optCD.state_dict(), opt_file)
            
#             # ä¿å­˜æœ€ä½³æ¨¡å‹
#             if is_best_model:
#                 best_gen_file = os.path.join(checkpoint_dir, "best_cd_model_gen.pth")
#                 torch.save(model_state_dict, best_gen_file)
                
#                 if hasattr(change_detection, 'optCD') and change_detection.optCD is not None:
#                     best_opt_file = os.path.join(checkpoint_dir, "best_cd_model_opt.pth")
#                     torch.save(change_detection.optCD.state_dict(), best_opt_file)
                
#                 print(f"ğŸ† [ä¸»è¿›ç¨‹] æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
            
#             print(f"ğŸ’¾ [ä¸»è¿›ç¨‹] Checkpointå·²ä¿å­˜: Epoch {epoch}")
            
#         except Exception as e:
#             print(f"âŒ [ä¸»è¿›ç¨‹] ä¿å­˜checkpointå¤±è´¥: {e}")
    
#     def load_checkpoint(self, change_detection, opt):
#         """åŠ è½½checkpoint"""
#         if self.is_main_process:
#             # ä¸»è¿›ç¨‹åŠ è½½
#             start_epoch, best_mF1 = load_checkpoint_if_exists(change_detection, opt)
#         else:
#             # éä¸»è¿›ç¨‹ç­‰å¾…
#             start_epoch, best_mF1 = 0, 0.0
        
#         # DDPéœ€è¦åŒæ­¥åŠ è½½çŠ¶æ€
#         if self.config.strategy == 'ddp':
#             # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹
#             dist.barrier()
            
#             # å¹¿æ’­åŠ è½½çŠ¶æ€ï¼ˆä»ä¸»è¿›ç¨‹åˆ°æ‰€æœ‰è¿›ç¨‹ï¼‰
#             state = torch.tensor([start_epoch, best_mF1], device=f'cuda:{self.config.local_rank}')
#             dist.broadcast(state, src=0)
#             start_epoch, best_mF1 = int(state[0].item()), float(state[1].item())
            
#             print(f"ğŸ“¡ [GPU {self.config.rank}] åŒæ­¥checkpointçŠ¶æ€: epoch={start_epoch}, best_mF1={best_mF1:.4f}")
        
#         return start_epoch, best_mF1

# # ==================== å¤šGPUè®­ç»ƒä¸»å‡½æ•° ====================
# def train_multi_gpu(rank, world_size, args, opt):
#     """å¤šGPUè®­ç»ƒä¸»å‡½æ•°"""
#     # 1. æ·»åŠ è°ƒè¯•ä¿¡æ¯
#     print(f"\nğŸš€ [GPU {rank}] å¼€å§‹å¤šGPUè®­ç»ƒï¼Œä¸–ç•Œå¤§å°: {world_size}")
    
#     # é…ç½®å¤šGPU
#     config = MultiGPUConfig(args, opt)
#     config.rank = rank
#     config.local_rank = rank
#     config.world_size = world_size
    
#     # 2. è°ƒè¯•åˆ†å¸ƒå¼çŠ¶æ€
#     debug_distributed_state(rank)
    
#     # 3. åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
#     if config.strategy == 'ddp':
#         print(f"ğŸ”„ [GPU {rank}] åˆå§‹åŒ–DDP...")
#         if not setup_distributed_training(rank, world_size):
#             print(f"âŒ [GPU {rank}] DDPåˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡º")
#             return
        
#         # å†æ¬¡è°ƒè¯•çŠ¶æ€
#         debug_distributed_state(rank)
    
#     try:
#         # è®¾ç½®æ—¥å¿—ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
#         if config.rank == 0:
#             Logger.setup_logger(None, opt['path']['log'], 'train', level=logging.INFO, screen=True)
#             Logger.setup_logger('test', opt['path']['log'], 'test', level=logging.INFO)
#             logger = logging.getLogger('base')
#             logger.info(Logger.dict2str(opt))
#             tb_logger = SummaryWriter(log_dir=opt['path']['tb_logger'])
#         else:
#             logger = None
#             tb_logger = None
        
#         # åˆå§‹åŒ–WandbLoggerï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
#         wandb_logger = None
#         if opt['enable_wandb'] and config.rank == 0:
#             import wandb
#             print("Initializing wandblog.")
#             wandb_logger = WandbLogger(opt)
#             wandb.define_metric('epoch')
#             wandb.define_metric('training/train_step')
#             wandb.define_metric("training/*", step_metric="train_step")
#             wandb.define_metric('validation/val_step')
#             wandb.define_metric("validation/*", step_metric="val_step")
        
#         # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
#         print(f"ğŸ”„ [GPU {rank}] åŠ è½½æ•°æ®é›†...")
#         train_loader = val_loader = test_loader = None
#         train_sampler = val_sampler = test_sampler = None
        
#         for phase, dataset_opt in opt['datasets'].items():
#             if phase == 'train' and args.phase != 'test':
#                 print(f"[GPU {rank}] Creating [train] change-detection dataloader.")
#                 train_set = Data.create_cd_dataset(dataset_opt, phase)
#                 train_loader, train_sampler = create_multi_gpu_dataloader(train_set, dataset_opt, phase, config)
#                 opt['len_train_dataloader'] = len(train_loader)

#             elif phase == 'val' and args.phase != 'test':
#                 print(f"[GPU {rank}] Creating [val] change-detection dataloader.")
#                 val_set = Data.create_cd_dataset(dataset_opt, phase)
#                 val_loader, val_sampler = create_multi_gpu_dataloader(val_set, dataset_opt, phase, config)
#                 opt['len_val_dataloader'] = len(val_loader)
            
#             elif phase == 'test' and args.phase == 'test':
#                 print(f"[GPU {rank}] Creating [test] change-detection dataloader.")
#                 test_set = Data.create_cd_dataset(dataset_opt, phase)
#                 test_loader, test_sampler = create_multi_gpu_dataloader(test_set, dataset_opt, phase, config)
#                 opt['len_test_dataloader'] = len(test_loader)
        
#         if config.rank == 0:
#             logger.info('Initial Dataset Finished')
        
#         # åŠ è½½æ‰©æ•£æ¨¡å‹
#         print(f"ğŸ”„ [GPU {rank}] åŠ è½½æ‰©æ•£æ¨¡å‹...")
#         diffusion = Model.create_model(opt)
        
#         # âš ï¸ é‡è¦ï¼šå…ˆè®¾ç½®å™ªå£°è°ƒåº¦ï¼Œå†è¿›è¡Œå¤šGPUåŒ…è£…
#         print(f"ğŸ”„ [GPU {rank}] è®¾ç½®å™ªå£°è°ƒåº¦...")
#         diffusion.set_new_noise_schedule(
#             opt['model']['beta_schedule'][opt['phase']], schedule_phase=opt['phase'])
        
#         # å¤šGPUåŒ…è£…å™¨
#         model_wrapper = MultiGPUModelWrapper(config)
#         diffusion = model_wrapper.wrap_diffusion_model(diffusion)
        
#         if config.rank == 0:
#             logger.info('Initial Diffusion Model Finished')
        
#         # åˆ›å»ºå˜åŒ–æ£€æµ‹æ¨¡å‹
#         print(f"ğŸ”„ [GPU {rank}] åŠ è½½å˜åŒ–æ£€æµ‹æ¨¡å‹...")
#         change_detection = Model.create_CD_model(opt)
#         change_detection = model_wrapper.wrap_change_detection_model(change_detection)
        
#         # è®¾ç½®è®­ç»ƒä¼˜åŒ–
#         use_amp = setup_training_optimization(diffusion, change_detection)
        
#         # åˆ›å»ºå¤šGPUæ€§èƒ½ç›‘æ§å™¨
#         performance_monitor = MultiGPUPerformanceMonitor(config)
        
#         # åˆ›å»ºcheckpointç®¡ç†å™¨
#         checkpoint_manager = MultiGPUCheckpointManager(config)
        
#         if config.rank == 0:
#             print("ğŸš€ æ‰€æœ‰è®¾ç½®å®Œæˆï¼Œå¼€å§‹å¤šGPUè®­ç»ƒ...\n")
        
#         # è®­ç»ƒå¾ªç¯
#         n_epoch = opt['train']['n_epoch']
#         start_epoch, best_mF1 = checkpoint_manager.load_checkpoint(change_detection, opt)
        
#         if opt['phase'] == 'train':
#             for current_epoch in range(start_epoch, n_epoch):
#                 epoch_start_time = time.time()
                
#                 # è®¾ç½®epochï¼ˆç”¨äºDDPé‡‡æ ·å™¨ï¼‰
#                 if config.strategy == 'ddp' and train_sampler is not None:
#                     train_sampler.set_epoch(current_epoch)
                
#                 change_detection._clear_cache()
                
#                 if config.rank == 0:
#                     train_result_path = f'{opt["path"]["results"]}/train/{current_epoch}'
#                     os.makedirs(train_result_path, exist_ok=True)
                
#                 ################
#                 ### è®­ç»ƒé˜¶æ®µ ###
#                 ################
#                 if config.rank == 0:
#                     print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ Epoch {current_epoch}/{n_epoch-1}")
#                     message = f'å­¦ä¹ ç‡: {change_detection.optCD.param_groups[0]["lr"]:.7f}'
#                     logger.info(message)
                
#                 for current_step, train_data in enumerate(train_loader):
#                     step_start_time = time.time()
                    
#                     # é«˜æ•ˆæ‰¹é‡å¤„ç†
#                     process_batch_efficiently(train_data, diffusion, change_detection, opt, "train")
                    
#                     # å¤šGPUå®‰å…¨è®­ç»ƒæ­¥éª¤
#                     success = safe_multi_gpu_training_step(change_detection, config)
                    
#                     if not success:
#                         if config.rank == 0:
#                             print(f"è·³è¿‡æ­¥éª¤ {current_step}")
#                         continue
                    
#                     # è®°å½•æ€§èƒ½
#                     step_time = time.time() - step_start_time
#                     memory_mb = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0
#                     performance_monitor.log_step(step_time, memory_mb)
                    
#                     # ä¼˜åŒ–çš„æ—¥å¿—è¾“å‡ºï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
#                     if config.rank == 0:
#                         optimized_logging(current_step, current_epoch, n_epoch, train_loader, 
#                                         change_detection, logger, opt, "train", performance_monitor)
                    
#                     # ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
#                     if config.rank == 0:
#                         save_freq = max(opt['train']['train_print_freq'] * 2, len(train_loader) // 5)
#                         if current_step % save_freq == 0:
#                             try:
#                                 visuals = change_detection.get_current_visuals()
                                
#                                 device = train_data['A'].device
#                                 visuals['pred_cm'] = visuals['pred_cm'] * 2.0 - 1.0
#                                 visuals['gt_cm'] = visuals['gt_cm'] * 2.0 - 1.0
                                
#                                 pred_cm_expanded = visuals['pred_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
#                                 gt_cm_expanded = visuals['gt_cm'].unsqueeze(1).repeat(1, 3, 1, 1).to(device)
                                
#                                 grid_img = torch.cat((train_data['A'], train_data['B'], 
#                                                     pred_cm_expanded, gt_cm_expanded), dim=0)
#                                 grid_img = Metrics.tensor2img(grid_img)
                                
#                                 Metrics.save_img(grid_img, 
#                                     f'{train_result_path}/img_A_B_pred_gt_e{current_epoch}_b{current_step}.png')
#                             except Exception as e:
#                                 print(f"ä¿å­˜å¯è§†åŒ–å¤±è´¥: {e}")
                    
#                     # å®šæœŸå†…å­˜æ¸…ç†
#                     if current_step % 50 == 0:
#                         torch.cuda.empty_cache()
                
#                 # DDPåŒæ­¥
#                 if config.strategy == 'ddp':
#                     dist.barrier()
                
#                 ### è®­ç»ƒepochæ€»ç»“ ###
#                 if config.rank == 0:
#                     try:
#                         change_detection._collect_epoch_states()
#                         logs = change_detection.get_current_log()
                        
#                         epoch_time = time.time() - epoch_start_time
#                         message = f'[è®­ç»ƒ Epoch {current_epoch}] mF1={logs["epoch_acc"]:.5f}, ' \
#                                  f'ç”¨æ—¶={epoch_time/60:.1f}åˆ†é’Ÿ'
                        
#                         print(f"\nâœ… {message}")
#                         logger.info(message)
                        
#                         # è¯¦ç»†æŒ‡æ ‡
#                         for k, v in logs.items():
#                             tb_logger.add_scalar(f'train/{k}', v, current_epoch)
                        
#                         if wandb_logger:
#                             wandb_logger.log_metrics({
#                                 'training/mF1': logs['epoch_acc'],
#                                 'training/mIoU': logs['miou'],
#                                 'training/OA': logs['acc'],
#                                 'training/change-F1': logs['F1_1'],
#                                 'training/no-change-F1': logs['F1_0'],
#                                 'training/change-IoU': logs['iou_1'],
#                                 'training/no-change-IoU': logs['iou_0'],
#                                 'training/train_step': current_epoch,
#                                 'training/loss': logs.get('l_cd'),
#                             })
                            
#                     except Exception as e:
#                         print(f"è®­ç»ƒæŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
                
#                 change_detection._clear_cache()
#                 change_detection._update_lr_schedulers()
                
#                 ##################
#                 ### éªŒè¯é˜¶æ®µ ###
#                 ##################
#                 if current_epoch % opt['train']['val_freq'] == 0:
#                     if config.rank == 0:
#                         print(f"\nğŸ” å¼€å§‹éªŒè¯ Epoch {current_epoch}")
#                         val_result_path = f'{opt["path"]["results"]}/val/{current_epoch}'
#                         os.makedirs(val_result_path, exist_ok=True)
                    
#                     val_start_time = time.time()

#                     for current_step, val_data in enumerate(val_loader):
#                         with torch.no_grad():
#                             process_batch_efficiently(val_data, diffusion, change_detection, opt, "val")
#                             change_detection.test()
#                             change_detection._collect_running_batch_states()
                        
#                         # éªŒè¯æ—¥å¿—ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
#                         if config.rank == 0:
#                             optimized_logging(current_step, current_epoch, n_epoch, val_loader, 
#                                             change_detection, logger, opt, "val")
                    
#                     # DDPåŒæ­¥
#                     if config.strategy == 'ddp':
#                         dist.barrier()
                    
#                     ### éªŒè¯æ€»ç»“ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰###
#                     if config.rank == 0:
#                         try:
#                             change_detection._collect_epoch_states()
#                             logs = change_detection.get_current_log()
                            
#                             val_time = time.time() - val_start_time
#                             message = f'[éªŒè¯ Epoch {current_epoch}] mF1={logs["epoch_acc"]:.5f}, ' \
#                                     f'ç”¨æ—¶={val_time/60:.1f}åˆ†é’Ÿ'
                            
#                             print(f"âœ… {message}")
#                             logger.info(message)
                            
#                             for k, v in logs.items():
#                                 tb_logger.add_scalar(f'val/{k}', v, current_epoch)

#                             if wandb_logger:
#                                 wandb_logger.log_metrics({
#                                     'validation/mF1': logs['epoch_acc'],
#                                     'validation/mIoU': logs['miou'],
#                                     'validation/OA': logs['acc'],
#                                     'validation/change-F1': logs['F1_1'],
#                                     'validation/no-change-F1': logs['F1_0'],
#                                     'validation/change-IoU': logs['iou_1'],
#                                     'validation/no-change-IoU': logs['iou_0'],
#                                     'epoch': current_epoch,
#                                 })
                            
#                             # æ¨¡å‹ä¿å­˜é€»è¾‘
#                             if logs['epoch_acc'] > best_mF1:
#                                 is_best_model = True
#                                 best_mF1 = logs['epoch_acc']
#                                 print(f"ğŸ‰ æœ€ä½³æ¨¡å‹æ›´æ–°! mF1: {best_mF1:.5f}")
#                                 logger.info('[éªŒè¯] æœ€ä½³æ¨¡å‹æ›´æ–°ï¼Œä¿å­˜æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€')
#                             else:
#                                 is_best_model = False
#                                 logger.info('[éªŒè¯] ä¿å­˜å½“å‰æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€')

#                             # ä¿å­˜checkpoint
#                             checkpoint_manager.save_checkpoint(change_detection, current_epoch, is_best_model)
                            
#                         except Exception as e:
#                             print(f"éªŒè¯æŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
                    
#                     change_detection._clear_cache()
#                     if config.rank == 0:
#                         print(f"--- è¿›å…¥ä¸‹ä¸€ä¸ªEpoch ---\n")

#                 if wandb_logger and config.rank == 0:
#                     wandb_logger.log_metrics({'epoch': current_epoch})
                
#                 # Epochç»“æŸæ¸…ç†
#                 torch.cuda.empty_cache()
            
#             if config.rank == 0:
#                 print("ğŸ‰ å¤šGPUè®­ç»ƒå®Œæˆ!")
#                 logger.info('å¤šGPUè®­ç»ƒç»“æŸ')
    
#     finally:
#         # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
#         if config.strategy == 'ddp':
#             cleanup_distributed()

# # ==================== åŸæœ‰å‡½æ•°ä¿æŒä¸å˜ ====================

# # ==================== ä¼˜åŒ–ç‰ˆæ ‡ç­¾éªŒè¯å™¨ ====================
# class LabelValidator:
#     """é«˜æ•ˆæ ‡ç­¾éªŒè¯å™¨ - å•ä¾‹æ¨¡å¼"""
#     _instance = None
#     _initialized = False
    
#     def __new__(cls):
#         if cls._instance is None:
#             cls._instance = super().__new__(cls)
#         return cls._instance
    
#     def __init__(self):
#         if not self._initialized:
#             self.is_normalized = None
#             self.validation_done = False
#             self.label_stats = {}
#             LabelValidator._initialized = True
    
#     def validate_and_fix_labels(self, data, phase="train"):
#         """
#         é«˜æ•ˆæ ‡ç­¾éªŒè¯ - åªåœ¨ç¬¬ä¸€æ¬¡è¯¦ç»†æ£€æŸ¥ï¼Œåç»­å¿«é€Ÿå¤„ç†
#         """
#         if 'L' not in data:
#             return False
        
#         labels = data['L']
        
#         # å¿«é€Ÿé€šé“ï¼šå¦‚æœå·²ç»éªŒè¯è¿‡ï¼Œç›´æ¥å¤„ç†
#         if self.validation_done:
#             if self.is_normalized:
#                 data['L'] = (labels >= 0.5).long()
#             else:
#                 fixed_labels = labels.clone()
#                 unique_vals = torch.unique(labels)
#                 if 255 in unique_vals:
#                     fixed_labels[labels == 255] = 1
#                 data['L'] = torch.clamp(fixed_labels, 0, 1).long()
#             return True
        
#         # ç¬¬ä¸€æ¬¡è¯¦ç»†éªŒè¯
#         unique_vals = torch.unique(labels)
#         min_val = labels.min().item()
#         max_val = labels.max().item()
        
#         print(f"\nğŸ” [{phase}] æ ‡ç­¾éªŒè¯ï¼ˆä»…æ˜¾ç¤ºä¸€æ¬¡ï¼‰:")
#         print(f"   å½¢çŠ¶: {labels.shape}, æ•°æ®ç±»å‹: {labels.dtype}")
#         print(f"   å€¼èŒƒå›´: [{min_val}, {max_val}]")
#         print(f"   å”¯ä¸€å€¼: {unique_vals.tolist()}")
        
#         # åˆ¤æ–­æ ‡ç­¾ç±»å‹
#         self.is_normalized = (max_val <= 1.0 and min_val >= 0.0)
        
#         if self.is_normalized:
#             print(f"   ğŸ”§ æ£€æµ‹åˆ°å½’ä¸€åŒ–æ ‡ç­¾ï¼Œä½¿ç”¨é˜ˆå€¼äºŒå€¼åŒ–ï¼ˆé˜ˆå€¼=0.5ï¼‰")
#             fixed_labels = (labels >= 0.5).long()
#         else:
#             print(f"   ğŸ”§ æ£€æµ‹åˆ°æ ‡å‡†æ ‡ç­¾ï¼Œæ˜ å°„255â†’1")
#             fixed_labels = labels.clone()
#             if 255 in unique_vals:
#                 fixed_labels[labels == 255] = 1
#             fixed_labels = torch.clamp(fixed_labels, 0, 1).long()
        
#         # éªŒè¯ä¿®å¤ç»“æœ
#         final_unique = torch.unique(fixed_labels)
#         zero_count = (fixed_labels == 0).sum().item()
#         one_count = (fixed_labels == 1).sum().item()
#         total = zero_count + one_count
        
#         print(f"   âœ… ä¿®å¤å®Œæˆ: å”¯ä¸€å€¼{final_unique.tolist()}")
#         print(f"   ğŸ“Š åƒç´ åˆ†å¸ƒ: æ— å˜åŒ–={100*zero_count/total:.1f}%, æœ‰å˜åŒ–={100*one_count/total:.1f}%")
#         print(f"   âœ… æ ‡ç­¾éªŒè¯è®¾ç½®å®Œæˆï¼Œåç»­æ‰¹æ¬¡å°†å¿«é€Ÿå¤„ç†\n")
        
#         # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
#         self.label_stats = {
#             'zero_ratio': zero_count / total,
#             'one_ratio': one_count / total,
#             'is_normalized': self.is_normalized
#         }
        
#         data['L'] = fixed_labels
#         self.validation_done = True
#         return True

# # å…¨å±€æ ‡ç­¾éªŒè¯å™¨
# label_validator = LabelValidator()

# # ==================== å†…å­˜ç®¡ç†å·¥å…· ====================
# @contextmanager
# def memory_efficient_context():
#     """å†…å­˜ç®¡ç†ä¸Šä¸‹æ–‡"""
#     if torch.cuda.is_available():
#         torch.cuda.empty_cache()
#     try:
#         yield
#     finally:
#         if torch.cuda.is_available():
#             torch.cuda.empty_cache()

# # ==================== ä¼˜åŒ–ç‰ˆç‰¹å¾é‡æ’ ====================
# def apply_feature_reordering_optimized(f_A, f_B, train_data, change_detection, opt, phase="train"):
#     """
#     å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾é‡æ’æ–¹æ¡ˆ
#     """
#     try:
#         feat_scales = opt['model_cd']['feat_scales']  # [2, 5, 8, 11, 14]
#         cd_expected_order = sorted(feat_scales, reverse=True)  # [14, 11, 8, 5, 2]
        
#         # åªåœ¨ç¬¬ä¸€æ¬¡æ˜¾ç¤ºä¿¡æ¯
#         if not hasattr(apply_feature_reordering_optimized, '_logged'):
#             print("ğŸ¯ ä½¿ç”¨ä¼˜åŒ–çš„ç‰¹å¾é‡æ’æ–¹æ¡ˆ")
#             print("   ä¿æŒåŸå§‹å¤šå°ºåº¦é…ç½®çš„å®Œæ•´è¯­ä¹‰")
#             for i, scale in enumerate(cd_expected_order):
#                 print(f"     Block{i}: ä½¿ç”¨layer{scale}ç‰¹å¾")
#             apply_feature_reordering_optimized._logged = True
        
#         # é«˜æ•ˆé‡æ’ï¼šç›´æ¥åœ¨åŸåœ°ä¿®æ”¹
#         reordered_f_A = []
#         reordered_f_B = []
        
#         for fa, fb in zip(f_A, f_B):
#             if isinstance(fa, list) and len(fa) > max(feat_scales):
#                 timestep_A = [fa[scale] for scale in cd_expected_order]
#                 timestep_B = [fb[scale] for scale in cd_expected_order]
#                 reordered_f_A.append(timestep_A)
#                 reordered_f_B.append(timestep_B)
#             else:
#                 raise ValueError(f"ç‰¹å¾æ ¼å¼é”™è¯¯: æœŸæœ›listé•¿åº¦>{max(feat_scales)}, å®é™…{type(fa)}")
        
#         # æ¸…ç†åŸå§‹ç‰¹å¾é‡Šæ”¾å†…å­˜
#         del f_A, f_B
        
#         # ä½¿ç”¨é‡æ’åçš„ç‰¹å¾
#         change_detection.feed_data(reordered_f_A, reordered_f_B, train_data)
        
#         # æ¸…ç†é‡æ’åçš„ç‰¹å¾
#         del reordered_f_A, reordered_f_B
        
#         return True
        
#     except Exception as e:
#         print(f"âŒ ç‰¹å¾é‡æ’å¤±è´¥: {e}")
#         print("ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ¡ˆ...")
        
#         # ç®€åŒ–å›é€€æ–¹æ¡ˆ
#         target_layers = [12, 13, 14]
#         corrected_f_A = []
#         corrected_f_B = []
        
#         for fa, fb in zip(f_A, f_B):
#             timestep_A = [fa[i] for i in target_layers if i < len(fa)]
#             timestep_B = [fb[i] for i in target_layers if i < len(fb)]
#             corrected_f_A.append(timestep_A)
#             corrected_f_B.append(timestep_B)
        
#         del f_A, f_B
#         change_detection.feed_data(corrected_f_A, corrected_f_B, train_data)
#         del corrected_f_A, corrected_f_B
        
#         return False

# # ==================== è®­ç»ƒä¼˜åŒ–è®¾ç½® ====================
# def setup_training_optimization(diffusion, change_detection):
#     """è®¾ç½®è®­ç»ƒä¼˜åŒ–"""
#     print("ğŸš€ è®¾ç½®è®­ç»ƒä¼˜åŒ–...")
    
#     # å¯ç”¨CUDAä¼˜åŒ–
#     torch.backends.cudnn.benchmark = True
#     torch.backends.cudnn.deterministic = False
    
#     # æ£€æŸ¥æ··åˆç²¾åº¦æ”¯æŒ
#     use_amp = False
#     if torch.cuda.is_available():
#         try:
#             from torch.cuda.amp import autocast, GradScaler
#             use_amp = True
#             print("   âœ… æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ")
#         except ImportError:
#             print("   âš ï¸  ä¸æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ")
    
#     # è®¾ç½®diffusionæ¨¡å‹ä¸ºevalæ¨¡å¼ï¼ˆå¦‚æœä¸éœ€è¦è®­ç»ƒï¼‰
#     if hasattr(diffusion.netG, 'eval'):
#         diffusion.netG.eval()
#         print("   âœ… Diffusionæ¨¡å‹è®¾ç½®ä¸ºevalæ¨¡å¼")
    
#     # æ£€æŸ¥å¤šGPUè®¾ç½®
#     if torch.cuda.device_count() > 1:
#         print(f"   âœ… æ£€æµ‹åˆ°{torch.cuda.device_count()}ä¸ªGPU")
        
#         # æ˜¾ç¤ºGPUçŠ¶æ€
#         for i in range(torch.cuda.device_count()):
#             props = torch.cuda.get_device_properties(i)
#             memory_gb = props.total_memory / 1024**3
#             print(f"     GPU {i}: {props.name} ({memory_gb:.1f}GB)")
    
#     print("ğŸš€ è®­ç»ƒä¼˜åŒ–è®¾ç½®å®Œæˆ\n")
    
#     return use_amp

# # ==================== æ‰¹é‡å¤„ç†ä¼˜åŒ– ====================
# def process_batch_efficiently(train_data, diffusion, change_detection, opt, phase="train"):
#     """é«˜æ•ˆçš„æ‰¹é‡å¤„ç†"""
#     with memory_efficient_context():
#         # 1. å¿«é€Ÿæ ‡ç­¾éªŒè¯
#         label_validator.validate_and_fix_labels(train_data, phase)
        
#         # 2. ç‰¹å¾æå–
#         diffusion.feed_data(train_data)
        
#         # 3. æ”¶é›†ç‰¹å¾
#         f_A, f_B = [], []
#         for t in opt['model_cd']['t']:
#             fe_A_t, fd_A_t, fe_B_t, fd_B_t = diffusion.get_feats(t=t)
#             if opt['model_cd']['feat_type'] == "dec":
#                 f_A.append(fd_A_t)
#                 f_B.append(fd_B_t)
#                 del fe_A_t, fe_B_t  # ç«‹å³æ¸…ç†
#             else:
#                 f_A.append(fe_A_t)
#                 f_B.append(fe_B_t)
#                 del fd_A_t, fd_B_t  # ç«‹å³æ¸…ç†
        
#         # 4. ç‰¹å¾é‡æ’
#         apply_feature_reordering_optimized(f_A, f_B, train_data, change_detection, opt, phase)

# # ==================== ä¼˜åŒ–çš„æ—¥å¿—ç®¡ç† ====================
# def optimized_logging(current_step, current_epoch, n_epoch, loader, change_detection, 
#                      logger, opt, phase="train", performance_monitor=None):
#     """ä¼˜åŒ–çš„æ—¥å¿—è¾“å‡º"""
#     # åŠ¨æ€è°ƒæ•´æ—¥å¿—é¢‘ç‡
#     if phase == "train":
#         base_freq = opt['train'].get('train_print_freq', 10)
#         log_freq = max(base_freq, len(loader) // 1000)  # è‡³å°‘æ¯0.1%æ˜¾ç¤ºä¸€æ¬¡
#     else:
#         log_freq = max(1, len(loader) // 1000)  # æµ‹è¯•æ—¶æ¯0.1%æ˜¾ç¤ºä¸€æ¬¡
    
#     if current_step % log_freq == 0:
#         try:
#             logs = change_detection.get_current_log()
            
#             # åŸºç¡€ä¿¡æ¯
#             progress = f"[{current_epoch}/{n_epoch-1}]"
#             step_info = f"Step {current_step}/{len(loader)}"
#             metrics = f"Loss: {logs.get('l_cd', 0):.5f} mF1: {logs.get('running_acc', 0):.5f}"
            
#             # æ€§èƒ½ä¿¡æ¯
#             perf_info = ""
#             if performance_monitor:
#                 perf_info = f" | {performance_monitor.get_stats()}"
            
#             message = f"{progress} {step_info} {metrics}{perf_info}"
#             print(message)
            
#         except Exception as e:
#             print(f"æ—¥å¿—è¾“å‡ºé”™è¯¯: {e}")

# # ==================== checkpointæ¢å¤ ====================
# def load_checkpoint_if_exists(change_detection, opt):
#     """ä¿®æ­£ç‰ˆæœ¬ï¼šä¼˜å…ˆbestæ¨¡å‹ï¼Œä¿®å¤åŠ è½½æ¥å£"""
    
#     # ğŸ¯ æŒ‡å®šçš„checkpointè·¯å¾„
#     checkpoint_dir = os.path.expanduser("~/bowen/DDPM-CD/ddpm-cd/checkpoint/gvlm_cd_ddpm")
    
#     if not os.path.exists(checkpoint_dir):
#         print(f"ğŸ” Checkpointç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
#         return 0, 0.0
    
#     print(f"ğŸ” æ£€æŸ¥checkpointç›®å½•: {checkpoint_dir}")
    
#     import glob
#     import re
    
#     # ========================================
#     # ğŸ¥‡ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæ£€æŸ¥æœ€ä½³æ¨¡å‹
#     # ========================================
#     best_gen_file = os.path.join(checkpoint_dir, "best_cd_model_gen.pth")
#     best_opt_file = os.path.join(checkpoint_dir, "best_cd_model_opt.pth")
    
#     if os.path.exists(best_gen_file):
#         print("ğŸ† å‘ç°æœ€ä½³æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨æœ€ä½³æ¨¡å‹")
#         print(f"   æœ€ä½³æ¨¡å‹æ–‡ä»¶: {best_gen_file}")
#         print(f"   æœ€ä½³ä¼˜åŒ–å™¨æ–‡ä»¶: {best_opt_file}")
        
#         success = load_model_safe(change_detection, best_gen_file, best_opt_file)
        
#         if success:
#             print("âœ… æœ€ä½³æ¨¡å‹åŠ è½½æˆåŠŸ")
#             # ä»æœ€ä½³æ¨¡å‹å¼€å§‹ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ªè¾ƒé«˜çš„epochæˆ–ä»0å¼€å§‹
#             return 0, 0.8  # ä»epoch 0å¼€å§‹ï¼Œä½†best_mF1è®¾ç½®è¾ƒé«˜å€¼è¡¨ç¤ºè¿™æ˜¯å¥½æ¨¡å‹
#         else:
#             print("âŒ æœ€ä½³æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•æœ€æ–°epochæ¨¡å‹")
    
#     # ========================================
#     # ğŸ¥ˆ ç¬¬äºŒä¼˜å…ˆçº§ï¼šæŸ¥æ‰¾æœ€æ–°epochæ¨¡å‹
#     # ========================================
#     gen_files = glob.glob(os.path.join(checkpoint_dir, "cd_model_E*_gen.pth"))
    
#     if gen_files:
#         print(f"ğŸ” æ‰¾åˆ°çš„epochæ¨¡å‹æ–‡ä»¶: {[os.path.basename(f) for f in gen_files]}")
        
#         # æå–epochæ•°å­—å¹¶æ’åº
#         epochs = []
#         for f in gen_files:
#             match = re.search(r'cd_model_E(\d+)_gen\.pth', f)
#             if match:
#                 epochs.append(int(match.group(1)))
        
#         if epochs:
#             latest_epoch = max(epochs)
            
#             # æ„å»ºæ–‡ä»¶è·¯å¾„
#             gen_file = os.path.join(checkpoint_dir, f"cd_model_E{latest_epoch}_gen.pth")
#             opt_file = os.path.join(checkpoint_dir, f"cd_model_E{latest_epoch}_opt.pth")
            
#             print(f"ğŸ”„ ä½¿ç”¨æœ€æ–°epochæ¨¡å‹: Epoch {latest_epoch}")
#             print(f"   æ¨¡å‹æ–‡ä»¶: {gen_file}")
#             print(f"   ä¼˜åŒ–å™¨æ–‡ä»¶: {opt_file}")
            
#             success = load_model_safe(change_detection, gen_file, opt_file)
            
#             if success:
#                 print("âœ… æœ€æ–°epochæ¨¡å‹åŠ è½½æˆåŠŸ")
                
#                 # æ£€æŸ¥best_mF1ï¼ˆå¯ä»¥ä»æŸä¸ªè®°å½•æ–‡ä»¶è¯»å–ï¼Œæˆ–è®¾ç½®é»˜è®¤å€¼ï¼‰
#                 best_mF1 = 0.0
#                 if os.path.exists(best_gen_file):
#                     best_mF1 = 0.5  # å¦‚æœæœ‰bestæ–‡ä»¶ä½†åŠ è½½å¤±è´¥ï¼Œè®¾ç½®ä¸€ä¸ªä¸­ç­‰å€¼
                
#                 return latest_epoch + 1, best_mF1
#             else:
#                 print("âŒ æœ€æ–°epochæ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥")
    
#     print("ğŸ†• æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
#     return 0, 0.0


# def load_model_safe(change_detection, gen_file, opt_file):
#     """å®‰å…¨çš„æ¨¡å‹åŠ è½½æ–¹æ³• - å°è¯•å¤šç§åŠ è½½æ–¹å¼"""
    
#     if not os.path.exists(gen_file):
#         print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {gen_file}")
#         return False
    
#     print(f"ğŸ”„ å°è¯•åŠ è½½æ¨¡å‹: {os.path.basename(gen_file)}")
    
#     # ========================================
#     # æ–¹æ³•1: ç›´æ¥torch.load + æ‰‹åŠ¨è®¾ç½®state_dict
#     # ========================================
#     try:
#         print("   ğŸ”„ æ–¹æ³•1: ç›´æ¥torch.load")
#         checkpoint = torch.load(gen_file, map_location='cpu')
        
#         # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå¤„ç†DDP/DPåŒ…è£…ï¼‰
#         model = change_detection.netCD
#         if isinstance(model, (DDP, nn.DataParallel)):
#             model = model.module
        
#         if model is not None:
#             # æ£€æŸ¥checkpointç»“æ„
#             if isinstance(checkpoint, dict):
#                 print(f"   ğŸ“‹ Checkpoint keys: {list(checkpoint.keys())}")
                
#                 # å°è¯•ä¸åŒçš„key
#                 state_dict = None
#                 for key in ['model_state_dict', 'state_dict', 'model', 'netCD']:
#                     if key in checkpoint:
#                         state_dict = checkpoint[key]
#                         print(f"   âœ… ä½¿ç”¨key: {key}")
#                         break
                
#                 if state_dict is None:
#                     # ç›´æ¥ä½œä¸ºstate_dict
#                     state_dict = checkpoint
#                     print("   âœ… ç›´æ¥ä½œä¸ºstate_dict")
#             else:
#                 state_dict = checkpoint
#                 print("   âœ… Checkpointæ˜¯state_dict")
            
#             # åŠ è½½state_dict
#             model.load_state_dict(state_dict, strict=False)
#             print("   âœ… æ–¹æ³•1: æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
            
#             # å°è¯•åŠ è½½ä¼˜åŒ–å™¨
#             load_optimizer_safe(change_detection, opt_file)
            
#             return True
            
#     except Exception as e:
#         print(f"   âŒ æ–¹æ³•1å¤±è´¥: {e}")
    
#     # ========================================
#     # æ–¹æ³•2: å°è¯•æ— å‚æ•°load_network (è®¾ç½®è·¯å¾„)
#     # ========================================
#     try:
#         print("   ğŸ”„ æ–¹æ³•2: æ— å‚æ•°load_network")
        
#         # å°è¯•è®¾ç½®è·¯å¾„åˆ°optä¸­
#         if hasattr(change_detection, 'opt'):
#             # å¤‡ä»½åŸè·¯å¾„
#             original_path = change_detection.opt.get('path', {}).get('resume_state', '')
            
#             # è®¾ç½®æ–°è·¯å¾„
#             if 'path' not in change_detection.opt:
#                 change_detection.opt['path'] = {}
#             change_detection.opt['path']['resume_state'] = gen_file
            
#             # å°è¯•åŠ è½½
#             change_detection.load_network()
            
#             # æ¢å¤åŸè·¯å¾„
#             if original_path:
#                 change_detection.opt['path']['resume_state'] = original_path
            
#             print("   âœ… æ–¹æ³•2: åŠ è½½æˆåŠŸ")
#             load_optimizer_safe(change_detection, opt_file)
#             return True
            
#     except Exception as e:
#         print(f"   âŒ æ–¹æ³•2å¤±è´¥: {e}")
    
#     print("   âŒ æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥")
#     return False


# def load_optimizer_safe(change_detection, opt_file):
#     """å®‰å…¨çš„ä¼˜åŒ–å™¨åŠ è½½"""
#     if not os.path.exists(opt_file):
#         print("   âš ï¸  ä¼˜åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨")
#         return False

#     try:
#         print(f"   ğŸ”„ åŠ è½½ä¼˜åŒ–å™¨: {os.path.basename(opt_file)}")
#         opt_state = torch.load(opt_file, map_location='cpu')

#         optimizer = None
#         for attr_name in ['optCD', 'optimizer', 'opt_CD', 'optim']:
#             if hasattr(change_detection, attr_name):
#                 optimizer = getattr(change_detection, attr_name)
#                 if optimizer is not None:
#                     print(f"   ğŸ“‹ æ‰¾åˆ°ä¼˜åŒ–å™¨å±æ€§: {attr_name}")
#                     break

#         if optimizer is not None:
#             print(f"   â„¹ï¸  opt_state type: {type(opt_state)}")
#             actual_state_to_load = None
#             if isinstance(opt_state, dict):
#                 print(f"   â„¹ï¸  opt_state keys: {list(opt_state.keys())}")
#                 if 'optimizer' in opt_state: # Primary case based on your log
#                     actual_state_to_load = opt_state['optimizer']
#                     print(f"   â„¹ï¸  ä½¿ç”¨ opt_state['optimizer'] è¿›è¡ŒåŠ è½½")
#                 elif 'state_dict' in opt_state: # Fallback for another common pattern
#                     actual_state_to_load = opt_state['state_dict']
#                     print(f"   â„¹ï¸  ä½¿ç”¨ opt_state['state_dict'] è¿›è¡ŒåŠ è½½")
#                 else: # Fallback: opt_state itself might be the state_dict
#                     actual_state_to_load = opt_state
#                     print(f"   â„¹ï¸  ç›´æ¥ä½¿ç”¨ opt_state è¿›è¡ŒåŠ è½½")
#             else: # opt_state is not a dict, assume it's the state_dict
#                 actual_state_to_load = opt_state
#                 print(f"   â„¹ï¸  opt_state ä¸æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨ opt_state è¿›è¡ŒåŠ è½½")

#             if actual_state_to_load is not None:
#                 optimizer.load_state_dict(actual_state_to_load)
#                 print("   âœ… ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
#                 return True
#             else:
#                 print("   âŒ æœªèƒ½ä» opt_state ä¸­ç¡®å®šè¦åŠ è½½çš„ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸")
#                 return False
#         else:
#             if opt['phase'] == 'test' and optimizer is None: # å¦‚æœæ˜¯æµ‹è¯•ä¸”æ²¡æ‰¾åˆ°å±æ€§
#                 print("   â„¹ï¸  æµ‹è¯•é˜¶æ®µï¼Œæœªæ‰¾åˆ°ä¼˜åŒ–å™¨å±æ€§ï¼Œè¿™é€šå¸¸æ˜¯æ­£å¸¸çš„ã€‚")
#             elif opt['phase'] == 'test' and optimizer is not None: # æ‰¾åˆ°äº†å±æ€§ä½†ä¸æ˜¯ä¼˜åŒ–å™¨å®ä¾‹
#                 print(f"   â„¹ï¸  æµ‹è¯•é˜¶æ®µï¼Œæ‰¾åˆ°å±æ€§ {attr_name} ä½†å®ƒä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ä¼˜åŒ–å™¨å®ä¾‹ã€‚")
#             else: # å…¶ä»–æƒ…å†µï¼ŒåŒ…æ‹¬è®­ç»ƒæ—¶æ²¡æ‰¾åˆ°
#                 print("   âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¼˜åŒ–å™¨å±æ€§æˆ–å®ä¾‹ã€‚")
#             return True # æˆ–è€… Trueï¼Œå–å†³äºä½ æ˜¯å¦è®¤ä¸ºè¿™æ˜¯ä¸ªé”™è¯¯

#     except Exception as e:
#         print(f"   âŒ ä¼˜åŒ–å™¨åŠ è½½å¤±è´¥: {e}")
#         return False

# # ==================== æ›´æ–°ä¸»å‡½æ•° ====================
# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument('-c', '--config', type=str, default='config/ddpm_cd.json',
#                         help='JSON file for configuration')
#     parser.add_argument('-p', '--phase', type=str, choices=['train', 'test'],
#                         help='Run either train(training + validation) or testing', default='train')
#     parser.add_argument('-gpu', '--gpu_ids', type=str, default=None)
#     parser.add_argument('-debug', '-d', action='store_true')
#     parser.add_argument('-enable_wandb', action='store_true')
#     parser.add_argument('-log_eval', action='store_true')
    
#     # æ–°å¢å¤šGPUé€‰é¡¹
#     parser.add_argument('--use_ddp', action='store_true', 
#                         help='ä½¿ç”¨DistributedDataParallel (æ¨è)')
#     parser.add_argument('--port', type=str, default='29500',
#                         help='åˆ†å¸ƒå¼è®­ç»ƒç«¯å£')
#     parser.add_argument('--multi_gpu_strategy', type=str, 
#                         choices=['auto', 'dp', 'ddp'], default='auto',
#                         help='å¤šGPUç­–ç•¥: auto(è‡ªåŠ¨é€‰æ‹©), dp(DataParallel), ddp(DistributedDataParallel)')

#     args = parser.parse_args()
    
#     # è§£æé…ç½®
#     opt = Logger.parse(args)
#     opt = Logger.dict_to_nonedict(opt)
    
#     # è®¾ç½®å¤šGPUç¯å¢ƒ
#     if torch.cuda.is_available():
#         world_size = torch.cuda.device_count()
#         print(f"ğŸ” æ£€æµ‹åˆ° {world_size} ä¸ªGPU")
        
#         # æ ¹æ®å‚æ•°å†³å®šè®­ç»ƒç­–ç•¥
#         if args.multi_gpu_strategy == 'ddp' or args.use_ddp:
#             args.use_ddp = True
#         elif args.multi_gpu_strategy == 'dp':
#             args.use_ddp = False
#         else:  # auto
#             # è‡ªåŠ¨é€‰æ‹©ï¼šGPUæ•°é‡>1æ—¶ä¼˜å…ˆä½¿ç”¨DDP
#             args.use_ddp = (world_size > 1)
        
#         if world_size > 1:
#             if args.use_ddp:
#                 print(f"ğŸš€ å¯åŠ¨å¤šGPUè®­ç»ƒ: DistributedDataParallel ({world_size} GPUs)")
#                 # å¯åŠ¨å¤šè¿›ç¨‹åˆ†å¸ƒå¼è®­ç»ƒ
#                 # mp.spawn(train_multi_gpu, 
#                 #         args=(world_size, args, opt), 
#                 #         nprocs=world_size, 
#                 #         join=True)
#                 # å¼ºåˆ¶ä½¿ç”¨DataParallel
#                 args.use_ddp = False
#                 args.multi_gpu_strategy = 'dp'
#                 config = MultiGPUConfig(args, opt)
#                 train_multi_gpu(0, world_size, args, opt)
#             else:
#                 print(f"ğŸš€ å¯åŠ¨å¤šGPUè®­ç»ƒ: DataParallel ({world_size} GPUs)")
#                 # å•è¿›ç¨‹DataParallelè®­ç»ƒ
#                 config = MultiGPUConfig(args, opt)
#                 train_multi_gpu(0, world_size, args, opt)
#         else:
#             print("ğŸš€ å¯åŠ¨å•GPUè®­ç»ƒ")
#             # å•GPUè®­ç»ƒ
#             config = MultiGPUConfig(args, opt)
#             train_multi_gpu(0, 1, args, opt)
#     else:
#         print("âŒ æœªæ£€æµ‹åˆ°GPUï¼Œé€€å‡ºè®­ç»ƒ")
#         exit(1)