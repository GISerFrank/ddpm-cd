"""
ç‰©ç†å¼•å¯¼çš„æ³¨æ„åŠ›æ¨¡å—ï¼ˆç­–ç•¥ä¸€ï¼‰- DDPM-Awareç‰ˆæœ¬
ä½¿ç”¨ç‰©ç†ç‰¹å¾ä½œä¸ºæ³¨æ„åŠ›æƒé‡æ¥å¢å¼ºè§†è§‰ç‰¹å¾

å…³é”®æ”¹åŠ¨ï¼š
1. BatchNorm2d â†’ GroupNorm (é€‚é…DDPMç‰¹å¾åˆ†å¸ƒ)
2. ReLU â†’ SiLU (DDPMæ ‡å‡†æ¿€æ´»å‡½æ•°)
3. å…¶ä»–é€»è¾‘ä¿æŒå®Œå…¨ç›¸åŒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class PhysicsGuidedAttention(nn.Module):
    """
    ç‰©ç†å¼•å¯¼çš„æ³¨æ„åŠ›æ¨¡å—ï¼ˆç­–ç•¥ä¸€ï¼‰- DDPM-Awareç‰ˆæœ¬
    ä½¿ç”¨ç‰©ç†ç‰¹å¾ä½œä¸ºæ³¨æ„åŠ›æƒé‡æ¥å¢å¼ºè§†è§‰ç‰¹å¾
    
    æ”¹åŠ¨ï¼š
    - BatchNorm â†’ GroupNorm (num_groups=8, DDPMæ ‡å‡†)
    - ReLU â†’ SiLU (DDPMæ ‡å‡†æ¿€æ´»)
    - æ¥æ”¶å°ºåº¦å¯¹é½çš„ç‰©ç†ç‰¹å¾ï¼ˆæ¥è‡ªé‡‘å­—å¡”ç¼–ç å™¨ï¼‰
    """
    
    def __init__(self, visual_channels, physical_channels, hidden_dim=64, 
                 dropout=0.1, num_groups=8):
        """
        Args:
            visual_channels: è§†è§‰ç‰¹å¾é€šé“æ•°
            physical_channels: ç‰©ç†ç‰¹å¾é€šé“æ•°ï¼ˆä¸visual_channelsç›¸åŒï¼Œæ¥è‡ªé‡‘å­—å¡”ï¼‰
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutç‡
            num_groups: GroupNormçš„ç»„æ•°
        """
        super().__init__()
        
        self.visual_channels = visual_channels
        self.physical_channels = physical_channels
        self.hidden_dim = hidden_dim
        self.num_groups = num_groups
        
        # ç‰©ç†ç‰¹å¾å·²ç»ä»é‡‘å­—å¡”ç¼–ç å™¨ç¼–ç å¥½ï¼Œåªéœ€ç®€å•å¤„ç†
        self.physics_refine = nn.Sequential(
            nn.Conv2d(physical_channels, hidden_dim, 3, padding=1),
            nn.GroupNorm(num_groups, hidden_dim),
            nn.SiLU()
        )
        
        # æ³¨æ„åŠ›ç”Ÿæˆå™¨ - DDPM-aware
        self.attention_conv = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim // 2, 1),
            nn.SiLU(),  # âœ… ReLU â†’ SiLU
            nn.Conv2d(hidden_dim // 2, 1, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾è°ƒåˆ¶å™¨ - DDPM-aware
        self.feature_modulator = nn.Sequential(
            nn.Conv2d(visual_channels + hidden_dim, visual_channels, 1),
            nn.GroupNorm(num_groups, visual_channels),  # âœ… BatchNorm â†’ GroupNorm
            nn.SiLU()                                    # âœ… ReLU â†’ SiLU
        )
        
        self.dropout = nn.Dropout2d(dropout)
        
    def forward(self, visual_features, physical_features):
        """
        Args:
            visual_features: [B, C_visual, H, W] - æ¥è‡ªDDPMçš„è§†è§‰ç‰¹å¾
            physical_features: [B, C_visual, H, W] - å°ºåº¦å¯¹é½çš„ç‰©ç†ç‰¹å¾ï¼ˆæ¥è‡ªé‡‘å­—å¡”ç¼–ç å™¨ï¼‰
            
        Returns:
            enhanced_features: [B, C_visual, H, W] - ç‰©ç†å¼•å¯¼å¢å¼ºåçš„ç‰¹å¾
            attention_map: [B, 1, H, W] - æ³¨æ„åŠ›å›¾ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        """
        B, C, H, W = visual_features.shape
        
        # ç‰©ç†ç‰¹å¾å·²ç»ä»é‡‘å­—å¡”ç¼–ç å™¨å¾—åˆ°ï¼Œå°ºå¯¸å’Œé€šé“æ•°éƒ½å¯¹é½ï¼Œæ— éœ€æ’å€¼
        
        # ç²¾ç‚¼ç‰©ç†ç‰¹å¾
        physics_refined = self.physics_refine(physical_features)
        physics_refined = self.dropout(physics_refined)
        
        # ç”Ÿæˆç©ºé—´æ³¨æ„åŠ›å›¾
        attention_map = self.attention_conv(physics_refined)  # [B, 1, H, W]
        
        # åº”ç”¨æ³¨æ„åŠ›åˆ°è§†è§‰ç‰¹å¾
        attended_visual = visual_features * attention_map
        
        # å°†ç‰©ç†ç‰¹å¾ä¿¡æ¯èåˆåˆ°è§†è§‰ç‰¹å¾ä¸­
        combined = torch.cat([attended_visual, physics_refined], dim=1)
        enhanced_features = self.feature_modulator(combined)
        
        # æ®‹å·®è¿æ¥
        enhanced_features = enhanced_features + visual_features
        
        return enhanced_features, attention_map


# ========== æµ‹è¯•ä»£ç  ==========
if __name__ == "__main__":
    """æµ‹è¯•DDPM-awareç‰ˆæœ¬çš„ç‰©ç†æ³¨æ„åŠ›ï¼ˆé…åˆé‡‘å­—å¡”ç¼–ç å™¨ï¼‰"""
    
    print("ğŸ§ª æµ‹è¯• PhysicsGuidedAttention (DDPM-aware + é‡‘å­—å¡”)")
    
    # åˆ›å»ºæ¨¡å—
    attention = PhysicsGuidedAttention(
        visual_channels=256,
        physical_channels=256,  # ä¸visualå¯¹é½
        hidden_dim=64,
        num_groups=8
    )
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 2
    H, W = 64, 64
    visual_feat = torch.randn(batch_size, 256, H, W)
    physical_feat = torch.randn(batch_size, 256, H, W)  # æ¥è‡ªé‡‘å­—å¡”ç¼–ç å™¨ï¼Œå·²å¯¹é½
    
    # å‰å‘ä¼ æ’­
    enhanced, attn_map = attention(visual_feat, physical_feat)
    
    print(f"âœ… è¾“å…¥è§†è§‰ç‰¹å¾: {visual_feat.shape}")
    print(f"âœ… è¾“å…¥ç‰©ç†ç‰¹å¾: {physical_feat.shape} (æ¥è‡ªé‡‘å­—å¡”ï¼Œå·²å¯¹é½)")
    print(f"âœ… è¾“å‡ºå¢å¼ºç‰¹å¾: {enhanced.shape}")
    print(f"âœ… è¾“å‡ºæ³¨æ„åŠ›å›¾: {attn_map.shape}")
    
    # æ£€æŸ¥æ¨¡å—ç±»å‹
    has_groupnorm = any('GroupNorm' in str(type(m)) for m in attention.modules())
    has_silu = any('SiLU' in str(type(m)) for m in attention.modules())
    has_batchnorm = any('BatchNorm' in str(type(m)) for m in attention.modules())
    has_relu = any(isinstance(m, nn.ReLU) for m in attention.modules())
    
    print(f"\næ¨¡å—æ£€æŸ¥:")
    print(f"  - GroupNorm: {'âœ…' if has_groupnorm else 'âŒ'}")
    print(f"  - SiLU: {'âœ…' if has_silu else 'âŒ'}")
    print(f"  - BatchNorm: {'âŒ' if not has_batchnorm else 'âš ï¸ ä»ç„¶å­˜åœ¨ï¼'}")
    print(f"  - ReLU: {'âŒ' if not has_relu else 'âš ï¸ ä»ç„¶å­˜åœ¨ï¼'}")
    
    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DDPM-awareç‰©ç†æ³¨æ„åŠ›æ¨¡å—å·¥ä½œæ­£å¸¸ï¼")
