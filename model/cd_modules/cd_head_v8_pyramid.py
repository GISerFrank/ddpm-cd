"""
cd_head_v8_pyramid - é€‚é…ç‰©ç†ç‰¹å¾é‡‘å­—å¡” + MoE
äº”é˜¶æ®µæ™ºèƒ½æ¨ç†æµç¨‹ï¼Œä½¿ç”¨å¤šå°ºåº¦ç‰©ç†ç‰¹å¾å’Œæ¡ä»¶åŒ–ä¸“å®¶å†³ç­–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from model.cd_modules.psp import _PSPModule
from model.cd_modules.se import ChannelSpatialSELayer
from model.cd_modules.cross_attention import MultiScaleCrossAttention
from model.cd_modules.mamba_mixer import ChangeDetectionMamba
from model.cd_modules.physics_focus_attention import PhysicsChangeFocusAttention

# å¯¼å…¥ç‰©ç†é‡‘å­—å¡”æ¨¡å—
from model.cd_modules.physical_encoder_pyramid import PhysicalFeaturePyramid
from model.cd_modules.physics_attention_pyramid import PhysicsGuidedAttention

# ğŸ†• å¯¼å…¥ MoE æ¨¡å—ï¼ˆç¬¬äº”é˜¶æ®µï¼‰
# æ¡ä»¶æ˜ å°„: 0=rainfall, 1=seismic, 2=snowmelt, 3=flood, 4=compound
from model.cd_modules.conditional_moe import MultiScaleMoE, create_condition_embedding


def get_in_channels(feat_scales, inner_channel, channel_multiplier):
    '''Get the number of input channels for each scale'''
    in_channels = 0
    for scale in feat_scales:
        if scale < 3:
            in_channels += inner_channel * channel_multiplier[0]
        elif scale < 6:
            in_channels += inner_channel * channel_multiplier[1]
        elif scale < 9:
            in_channels += inner_channel * channel_multiplier[2]
        elif scale < 12:
            in_channels += inner_channel * channel_multiplier[3]
        elif scale < 15:
            in_channels += inner_channel * channel_multiplier[4]
        else:
            print(f'Unbounded number for feat_scales: {scale}')
    return in_channels


class PhysicsEnhancedBlock(nn.Module):
    """
    ç‰©ç†å¢å¼ºBlock - DDPM-awareç‰ˆæœ¬
    é›†æˆç‰©ç†å¼•å¯¼æ³¨æ„åŠ›ï¼ˆç­–ç•¥ä¸€ï¼‰
    """
    def __init__(self, dim, dim_out, time_steps, physics_attention=None, num_groups=8):
        super().__init__()
        
        # æ—¶é—´æ­¥èåˆ - DDPM-aware
        self.time_fusion = nn.Sequential(
            nn.Conv2d(dim * len(time_steps), dim, 1) if len(time_steps) > 1 else nn.Identity(),
            nn.GroupNorm(num_groups, dim) if len(time_steps) > 1 else nn.Identity(),
            nn.SiLU() if len(time_steps) > 1 else nn.Identity(),
        )
        
        # ç‰©ç†å¼•å¯¼æ³¨æ„åŠ›ï¼ˆç­–ç•¥ä¸€ï¼‰
        self.physics_attention = physics_attention
        
        # ç‰¹å¾å¤„ç† - DDPM-aware
        self.feature_conv = nn.Sequential(
            nn.Conv2d(dim, dim_out, 3, padding=1),
            nn.GroupNorm(num_groups, dim_out),
            nn.SiLU()
        )
        
    def forward(self, x, physical_features=None):
        """
        Args:
            x: æ‹¼æ¥çš„æ—¶é—´æ­¥ç‰¹å¾
            physical_features: å°ºåº¦å¯¹é½çš„ç‰©ç†ç‰¹å¾ï¼ˆæ¥è‡ªé‡‘å­—å¡”ï¼‰
        """
        # æ—¶é—´æ­¥èåˆ
        x = self.time_fusion(x)
        
        # åº”ç”¨ç‰©ç†å¼•å¯¼æ³¨æ„åŠ›
        if self.physics_attention is not None and physical_features is not None:
            x, _ = self.physics_attention(x, physical_features)
        
        # ç‰¹å¾å¤„ç†
        x = self.feature_conv(x)
        return x


class AttentionBlock(nn.Module):
    """ä¿æŒåŸæœ‰çš„AttentionBlockç»“æ„"""
    def __init__(self, dim, dim_out):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(dim, dim_out, 3, padding=1),
            nn.ReLU(),
            ChannelSpatialSELayer(num_channels=dim_out, reduction_ratio=2)
        )

    def forward(self, x):
        return self.block(x)


class cd_head_v8_pyramid(nn.Module):
    '''
    Change detection head v8 - ç‰©ç†é‡‘å­—å¡” + MoE ç‰ˆæœ¬
    
    äº”é˜¶æ®µæ™ºèƒ½æ¨ç†ï¼š
    ç¬¬ä¸€é˜¶æ®µï¼šç‰©ç†å¼•å¯¼çš„çŠ¶æ€ç†è§£ (å°ºåº¦ç‰¹å®šç‰©ç†ç‰¹å¾) âœ“
    ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½çš„äº¤äº’å¼å¯¹æ¯”ï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰âœ“
    ç¬¬ä¸‰é˜¶æ®µï¼šå…¨å±€å½¢æ€å­¦åˆ†æï¼ˆMambaï¼‰âœ“
    ç¬¬å››é˜¶æ®µï¼šç‰©ç†å¼•å¯¼çš„å˜åŒ–èšç„¦ (å°ºåº¦ç‰¹å®šç‰©ç†ç‰¹å¾) âœ“
    ç¬¬äº”é˜¶æ®µï¼šæ¡ä»¶åŒ–ä¸“å®¶å†³ç­–ï¼ˆMoEï¼‰âœ“ ğŸ†•
    '''

    def __init__(self, opt, physics_attention_config=None, cross_attention_config=None, 
                 mamba_config=None, physics_focus_config=None, moe_config=None, num_groups=8):
        """
        Args:
            opt: å®Œæ•´é…ç½®å­—å…¸
            physics_attention_config: ç‰©ç†æ³¨æ„åŠ›æ¨¡å—é…ç½®
            cross_attention_config: äº¤å‰æ³¨æ„åŠ›é…ç½®
            mamba_config: Mambaé…ç½®
            physics_focus_config: ç‰©ç†èšç„¦é…ç½®
            moe_config: MoEé…ç½® ğŸ†•
            num_groups: GroupNormçš„ç»„æ•°
        """
        super(cd_head_v8_pyramid, self).__init__()

        # ä»optè¯»å–åŸºç¡€å‚æ•°
        feat_scales = opt['model_cd']['feat_scales']
        out_channels = opt['model_cd']['out_channels']
        inner_channel = opt['model']['unet']['inner_channel']
        channel_multiplier = opt['model']['unet']['channel_multiplier']
        img_size = opt['model_cd']['output_cm_size']
        time_steps = opt['model_cd'].get('t', [0])
        
        # åŸºç¡€å‚æ•°è®¾ç½®
        feat_scales_sorted = sorted(feat_scales, reverse=True)
        self.feat_scales = feat_scales_sorted
        self.img_size = img_size
        self.time_steps = time_steps
        self.num_groups = num_groups
        
        # ç‰©ç†é…ç½®
        self.physics_attention_config = physics_attention_config or {}
        self.use_physics = self.physics_attention_config.get('enabled', False)
        
        # äº¤å‰æ³¨æ„åŠ›é…ç½®
        self.cross_attention_config = cross_attention_config or {}
        self.use_cross_attention = self.cross_attention_config.get('enabled', False)
        
        # Mambaé…ç½®
        self.mamba_config = mamba_config or {}
        self.use_mamba = self.mamba_config.get('enabled', False)
        
        # ç‰©ç†èšç„¦é…ç½®
        self.physics_focus_config = physics_focus_config or {}
        self.use_physics_focus = self.physics_focus_config.get('enabled', False) and self.use_physics
        
        # ğŸ†• MoEé…ç½®
        self.moe_config = moe_config or {}
        self.use_moe = self.moe_config.get('enabled', False)
        
        # ç‰©ç†ç‰¹å¾é‡‘å­—å¡”ç¼–ç å™¨
        if self.use_physics:
            num_physical_layers = self.physics_attention_config.get('num_physical_layers', 2)
            self.physical_pyramid = PhysicalFeaturePyramid(
                opt=opt,
                num_physical_layers=num_physical_layers,
                num_groups=num_groups
            )
        
        # è®¡ç®—æ¯ä¸ªå°ºåº¦çš„ç»´åº¦
        self.scale_dims = []
        for scale in self.feat_scales:
            dim = get_in_channels([scale], inner_channel, channel_multiplier)
            self.scale_dims.append(dim)
        
        # å¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰
        if self.use_cross_attention:
            self.cross_attention = MultiScaleCrossAttention(
                scale_dims=self.scale_dims,
                num_heads_list=self.cross_attention_config.get('num_heads_list', None),
                dropout=self.cross_attention_config.get('dropout', 0.1)
            )
        
        # Mambaå…¨å±€å½¢æ€åˆ†ææ¨¡å—ï¼ˆç¬¬ä¸‰é˜¶æ®µï¼‰
        if self.use_mamba:
            self.mamba_mixer = ChangeDetectionMamba(
                scale_dims=self.scale_dims,
                d_state=self.mamba_config.get('d_state', 16),
                d_conv=self.mamba_config.get('d_conv', 4),
                expand=self.mamba_config.get('expand', 2),
                n_layers=self.mamba_config.get('n_layers', 2),
                use_multi_direction=self.mamba_config.get('use_multi_direction', True)
            )
        
        # ğŸ†• æ¡ä»¶åŒ–ä¸“å®¶å†³ç­–æ¨¡å—ï¼ˆç¬¬äº”é˜¶æ®µï¼‰
        if self.use_moe:
            self.moe_layer = MultiScaleMoE(
                scale_dims=self.scale_dims,
                num_experts=self.moe_config.get('num_experts', 4),
                num_conditions=self.moe_config.get('num_conditions', 5),
                scale_specific_experts=self.moe_config.get('scale_specific_experts', None),
                temperature=self.moe_config.get('temperature', 1.0),
                use_load_balancing=self.moe_config.get('use_load_balancing', True),
                dropout=self.moe_config.get('dropout', 0.1)
            )
        
        # æ„å»ºè§£ç å™¨
        self.decoder = nn.ModuleList()
        current_decoder_output_channels = 0
        
        for i in range(len(self.feat_scales)):
            scale = self.feat_scales[i]
            dim = self.scale_dims[i]
            
            # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºç‰©ç†å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰
            physics_attention = None
            if self.use_physics:
                physics_attention = PhysicsGuidedAttention(
                    visual_channels=dim,
                    physical_channels=dim,  # ç‰©ç†ç‰¹å¾é€šé“æ•°ä¸è§†è§‰ç‰¹å¾å¯¹é½
                    hidden_dim=64,
                    dropout=self.physics_attention_config.get('dropout', 0.1),
                    num_groups=num_groups
                )
            
            # ä½¿ç”¨å¢å¼ºçš„Block
            self.decoder.append(
                PhysicsEnhancedBlock(
                    dim=dim,
                    dim_out=dim,
                    time_steps=self.time_steps,
                    physics_attention=physics_attention,
                    num_groups=num_groups
                )
            )
            current_block_output_channels = dim

            if i != len(self.feat_scales) - 1:
                dim_out_for_attention = get_in_channels(
                    [self.feat_scales[i + 1]], inner_channel, channel_multiplier
                )
                self.decoder.append(
                    AttentionBlock(dim=current_block_output_channels, dim_out=dim_out_for_attention)
                )
                current_decoder_output_channels = dim_out_for_attention
            else:
                current_decoder_output_channels = current_block_output_channels

        # ç‰©ç†èšç„¦æ¨¡å—ï¼ˆç¬¬å››é˜¶æ®µï¼‰
        if self.use_physics_focus:
            self.physics_focus_modules = nn.ModuleDict()
            for i, scale in enumerate(self.feat_scales):
                dim = self.scale_dims[i]
                self.physics_focus_modules[str(scale)] = PhysicsChangeFocusAttention(
                    change_channels=dim,
                    physical_channels=dim,  # ä¸è§†è§‰ç‰¹å¾å¯¹é½
                    hidden_dim=self.physics_focus_config.get('hidden_dim', 128),
                    dropout=self.physics_focus_config.get('dropout', 0.1),
                    num_groups=num_groups
                )
        
        # æœ€ç»ˆåˆ†ç±»å¤´
        clfr_emb_dim = 64
        self.clfr_stg1 = nn.Conv2d(current_decoder_output_channels, clfr_emb_dim, kernel_size=3, padding=1)
        self.clfr_stg2 = nn.Conv2d(clfr_emb_dim, out_channels, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        
        # æ³¨æ„åŠ›å›¾æ”¶é›†å™¨
        self.attention_maps = {}
        
        # ğŸ†• MoE è¾…åŠ©æŸå¤±ï¼ˆç”¨äºè®­ç»ƒï¼‰
        self.moe_aux_loss = None

    def forward(self, feats_A, feats_B, physical_data=None, condition_id=None):
        """
        äº”é˜¶æ®µæ™ºèƒ½æ¨ç†æµç¨‹
        
        Args:
            feats_A, feats_B: List[List[Tensor]] - DDPMç‰¹å¾
            physical_data: [B, num_layers, 256, 256] - åŸå§‹ç‰©ç†æ•°æ®
            condition_id: [B] tensor æˆ– str - æ»‘å¡è¯±å› æ¡ä»¶ ğŸ†•
                å¯é€‰å€¼: 'rainfall', 'seismic', 'snowmelt', 'flood', 'compound'
                æˆ–è€…ç›´æ¥ä¼ å…¥ tensor [B] æ¯ä¸ªå…ƒç´ æ˜¯ 0-4 çš„æ•´æ•°
        """
        batch_size = feats_A[0][0].shape[0]
        
        # ç”Ÿæˆç‰©ç†ç‰¹å¾é‡‘å­—å¡”
        pyramid_physics = None
        if self.use_physics and physical_data is not None:
            pyramid_physics = self.physical_pyramid(physical_data)
            # pyramid_physics: Dict[scale -> Tensor]
        
        # ğŸ†• å¤„ç†æ¡ä»¶IDï¼ˆç”¨äºMoEï¼‰
        if self.use_moe:
            if condition_id is None:
                # é»˜è®¤ä½¿ç”¨ 'compound' ç±»å‹
                condition_id = create_condition_embedding(
                    batch_size, 'compound', feats_A[0][0].device
                )
            elif isinstance(condition_id, str):
                # å­—ç¬¦ä¸²è½¬ tensor
                condition_id = create_condition_embedding(
                    batch_size, condition_id, feats_A[0][0].device
                )
        
        # æ¸…ç©ºæ³¨æ„åŠ›å›¾å’Œè¾…åŠ©æŸå¤±
        self.attention_maps = {}
        self.moe_aux_loss = None
        
        # è§£ç è¿‡ç¨‹
        lvl_idx = 0
        x = None
        
        for layer_idx, layer in enumerate(self.decoder):
            if isinstance(layer, PhysicsEnhancedBlock):
                current_scale = self.feat_scales[lvl_idx]
                
                # æ”¶é›†å½“å‰å°ºåº¦çš„æ‰€æœ‰æ—¶é—´æ­¥ç‰¹å¾
                if len(self.time_steps) > 1:
                    list_to_cat_A = [feats_A[t_idx][lvl_idx] for t_idx in range(len(self.time_steps))]
                    list_to_cat_B = [feats_B[t_idx][lvl_idx] for t_idx in range(len(self.time_steps))]
                    f_A_cat = torch.cat(list_to_cat_A, dim=1)
                    f_B_cat = torch.cat(list_to_cat_B, dim=1)
                else:
                    f_A_cat = feats_A[0][lvl_idx]
                    f_B_cat = feats_B[0][lvl_idx]
                
                # è·å–å½“å‰å°ºåº¦çš„ç‰©ç†ç‰¹å¾
                physics_for_scale = pyramid_physics[current_scale] if pyramid_physics else None
                
                # ========== ç¬¬ä¸€é˜¶æ®µï¼šç‹¬ç«‹çš„çŠ¶æ€ç†è§£ ==========
                processed_f_A = layer(f_A_cat, physics_for_scale)
                processed_f_B = layer(f_B_cat, physics_for_scale)
                
                # ========== ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½çš„äº¤äº’å¼å¯¹æ¯” ==========
                if self.use_cross_attention:
                    diff = self.cross_attention(processed_f_A, processed_f_B, lvl_idx)
                else:
                    diff = torch.abs(processed_f_A - processed_f_B)
                
                # ========== ç¬¬ä¸‰é˜¶æ®µï¼šå…¨å±€å½¢æ€å­¦åˆ†æï¼ˆMambaï¼‰==========
                if self.use_mamba:
                    diff = self.mamba_mixer(diff, lvl_idx)
                
                # ========== ç¬¬å››é˜¶æ®µï¼šç‰©ç†å¼•å¯¼çš„å˜åŒ–èšç„¦ ==========
                if self.use_physics_focus and physics_for_scale is not None:
                    focus_module = self.physics_focus_modules[str(current_scale)]
                    diff, focus_attention = focus_module(diff, physics_for_scale)
                    self.attention_maps[f'scale_{current_scale}'] = focus_attention
                
                # ========== ğŸ†• ç¬¬äº”é˜¶æ®µï¼šæ¡ä»¶åŒ–ä¸“å®¶å†³ç­–ï¼ˆMoEï¼‰==========
                if self.use_moe:
                    # åº”ç”¨MoEå±‚
                    diff, moe_aux_loss = self.moe_layer(diff, condition_id, lvl_idx)
                    
                    # ç´¯ç§¯è¾…åŠ©æŸå¤±ï¼ˆç”¨äºè®­ç»ƒæ—¶çš„è´Ÿè½½å‡è¡¡ï¼‰
                    if self.training:
                        if self.moe_aux_loss is None:
                            self.moe_aux_loss = moe_aux_loss
                        else:
                            self.moe_aux_loss += moe_aux_loss
                
                # ä¸ä¸Šä¸€å±‚èåˆ
                if x is not None:
                    if x.shape[2:] != diff.shape[2:]:
                        x = F.interpolate(x, size=diff.shape[2:], mode='bilinear', align_corners=False)
                    x = x + diff
                else:
                    x = diff
                
                lvl_idx += 1
                
            elif isinstance(layer, AttentionBlock):
                x = layer(x)
        
        # æœ€ç»ˆåˆ†ç±»
        x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)
        x = self.clfr_stg1(x)
        x = self.relu(x)
        pred = self.clfr_stg2(x)
        
        return pred
    
    def get_attention_maps(self):
        """è·å–ä¿å­˜çš„æ³¨æ„åŠ›å›¾ï¼ˆç”¨äºå¯è§†åŒ–åˆ†æï¼‰"""
        return self.attention_maps
    
    def get_moe_aux_loss(self):
        """ğŸ†• è·å–MoEè¾…åŠ©æŸå¤±ï¼ˆç”¨äºè®­ç»ƒï¼‰"""
        return self.moe_aux_loss if self.moe_aux_loss is not None else 0.0


# ========== æµ‹è¯•ä»£ç  ==========
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• cd_head_v8_pyramid with MoE")
    
    # æ¨¡æ‹Ÿconfig
    test_opt = {
        'model_cd': {
            'feat_scales': [14, 11, 8, 5],
            'out_channels': 2,
            'output_cm_size': 256,
            't': [50, 100, 400]
        },
        'model': {
            'unet': {
                'inner_channel': 128,
                'channel_multiplier': [1, 2, 4, 8, 8]
            }
        }
    }
    
    # ç‰©ç†é…ç½®
    physics_attention_config = {
        'enabled': True,
        'num_physical_layers': 2,
        'dropout': 0.1
    }
    
    # ğŸ†• MoEé…ç½®
    moe_config = {
        'enabled': True,
        'num_experts': 5,  # ğŸ”„ æ”¹ä¸º5ä¸ªä¸“å®¶
        'num_conditions': 5,  # rainfall, seismic, snowmelt, flood, compound
        'temperature': 1.0,
        'use_load_balancing': True,
        'dropout': 0.1,
        'scale_specific_experts': None
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = cd_head_v8_pyramid(
        opt=test_opt,
        physics_attention_config=physics_attention_config,
        moe_config=moe_config,
        num_groups=8
    )
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 2
    # æ¨¡æ‹ŸDDPMç‰¹å¾: List[List[Tensor]]
    # 3ä¸ªæ—¶é—´æ­¥ Ã— 4ä¸ªå°ºåº¦
    feats_A = [
        [torch.randn(batch_size, 1024, 16, 16),   # scale 14
         torch.randn(batch_size, 1024, 32, 32),   # scale 11
         torch.randn(batch_size, 512, 64, 64),    # scale 8
         torch.randn(batch_size, 256, 128, 128)]  # scale 5
        for _ in range(3)
    ]
    feats_B = [
        [torch.randn(batch_size, 1024, 16, 16),
         torch.randn(batch_size, 1024, 32, 32),
         torch.randn(batch_size, 512, 64, 64),
         torch.randn(batch_size, 256, 128, 128)]
        for _ in range(3)
    ]
    physical_data = torch.randn(batch_size, 2, 256, 256)
    
    # ğŸ†• æµ‹è¯•ä¸åŒçš„æ¡ä»¶
    print("\n" + "="*60)
    print("æµ‹è¯•1: ä½¿ç”¨é»˜è®¤æ¡ä»¶ (compound)")
    pred1 = model(feats_A, feats_B, physical_data)
    print(f"âœ… è¾“å‡ºé¢„æµ‹: {pred1.shape}")
    
    print("\næµ‹è¯•2: ä½¿ç”¨ 'rainfall' æ¡ä»¶")
    pred2 = model(feats_A, feats_B, physical_data, condition_id='rainfall')
    print(f"âœ… è¾“å‡ºé¢„æµ‹: {pred2.shape}")
    
    print("\næµ‹è¯•3: ä½¿ç”¨ tensor æ¡ä»¶")
    condition_tensor = torch.tensor([0, 1])  # rainfall, seismic
    pred3 = model(feats_A, feats_B, physical_data, condition_id=condition_tensor)
    print(f"âœ… è¾“å‡ºé¢„æµ‹: {pred3.shape}")
    
    print("\n" + "="*60)
    print(f"âœ… æ³¨æ„åŠ›å›¾æ•°é‡: {len(model.attention_maps)}")
    for scale, attn in model.attention_maps.items():
        print(f"  {scale}: {attn.shape}")
    
    print(f"\nğŸ†• MoEè¾…åŠ©æŸå¤±: {model.get_moe_aux_loss()}")
    
    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
